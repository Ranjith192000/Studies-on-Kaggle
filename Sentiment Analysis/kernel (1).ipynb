{
  "cells": [
    {
      "metadata": {
        "_uuid": "2a8f7168e29eb6c697a85584d55999ff2d294524"
      },
      "cell_type": "markdown",
      "source": "# Analyzing Movie Reviews - Sentiment Analysis\nIn this notebook, we focus on trying to analyze a large corpus of movie reviews and derive the sentiment.\n\n[![image](http://www.moviereviewworld.com/wp-content/uploads/2013/06/movie-review-world-homepage-image.jpg)](http://www.moviereviewworld.com/)\n\nWe cover a wide variety of techniques for analyzing sentiment, which include the following.\n- Unsupervised lexicon-based models\n- Traditional supervised Machine Learning models\n- Newer supervised Deep Learning models\n- Advanced supervised Deep Learning models\n\nBesides looking at various approaches and models, we also focus on important aspects in the Machine Learning pipeline including text pre-processing, normalization, and in-depth analysis of models, including model interpretation and topic models. The key idea here is to understand how we tackle a problem like sentiment analysis on unstructured text, learn various techniques, models and understand how to interpret the results. This will enable you to use these methodologies in the future on your own datasets. Let's get started!"
    },
    {
      "metadata": {
        "toc": true,
        "_uuid": "3dcbd68028680603976553dff3cef525718fd642"
      },
      "cell_type": "markdown",
      "source": "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-classify-Sentiment?\" data-toc-modified-id=\"How-to-classify-Sentiment?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>How to classify Sentiment?</a></span></li></ul></li><li><span><a href=\"#Preparing-environment-and-data\" data-toc-modified-id=\"Preparing-environment-and-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preparing environment and data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-and-Setting-Up-Dependencies\" data-toc-modified-id=\"Import-and-Setting-Up-Dependencies-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Import and Setting Up Dependencies</a></span></li><li><span><a href=\"#Text-Pre-Processing-and-Normalization\" data-toc-modified-id=\"Text-Pre-Processing-and-Normalization-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Text Pre-Processing and Normalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-Text---strip-HTML\" data-toc-modified-id=\"Cleaning-Text---strip-HTML-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Cleaning Text - strip HTML</a></span></li><li><span><a href=\"#Removing-accented-characters\" data-toc-modified-id=\"Removing-accented-characters-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Removing accented characters</a></span></li><li><span><a href=\"#Expanding-Contractions\" data-toc-modified-id=\"Expanding-Contractions-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Expanding Contractions</a></span></li><li><span><a href=\"#Removing-Special-Characters\" data-toc-modified-id=\"Removing-Special-Characters-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Removing Special Characters</a></span></li><li><span><a href=\"#Lemmatizing-text\" data-toc-modified-id=\"Lemmatizing-text-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Lemmatizing text</a></span></li><li><span><a href=\"#Removing-Stopwords\" data-toc-modified-id=\"Removing-Stopwords-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span>Removing Stopwords</a></span></li><li><span><a href=\"#Normalize-text-corpus---tying-it-all-together\" data-toc-modified-id=\"Normalize-text-corpus---tying-it-all-together-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;</span>Normalize text corpus - tying it all together</a></span></li></ul></li><li><span><a href=\"#Topics-Help-Functions\" data-toc-modified-id=\"Topics-Help-Functions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Topics Help Functions</a></span></li><li><span><a href=\"#Simplify-Get-Results\" data-toc-modified-id=\"Simplify-Get-Results-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Simplify Get Results</a></span></li><li><span><a href=\"#Load-and-normalize-data\" data-toc-modified-id=\"Load-and-normalize-data-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Load and normalize data</a></span></li></ul></li><li><span><a href=\"#Sentiment-Analysis---Unsupervised-Lexical\" data-toc-modified-id=\"Sentiment-Analysis---Unsupervised-Lexical-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Sentiment Analysis - Unsupervised Lexical</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentiment-Analysis-with-AFINN\" data-toc-modified-id=\"Sentiment-Analysis-with-AFINN-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Sentiment Analysis with AFINN</a></span></li><li><span><a href=\"#Sentiment-Analysis-with-SentiWordNet\" data-toc-modified-id=\"Sentiment-Analysis-with-SentiWordNet-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Sentiment Analysis with SentiWordNet</a></span></li><li><span><a href=\"#Sentiment-Analysis-with-VADER\" data-toc-modified-id=\"Sentiment-Analysis-with-VADER-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Sentiment Analysis with VADER</a></span></li></ul></li><li><span><a href=\"#Classifying-Sentiment-with-Supervised-Learning\" data-toc-modified-id=\"Classifying-Sentiment-with-Supervised-Learning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Classifying Sentiment with Supervised Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Feature Engineering</a></span></li><li><span><a href=\"#Traditional-Supervised-Machine-Learning-Models\" data-toc-modified-id=\"Traditional-Supervised-Machine-Learning-Models-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Traditional Supervised Machine Learning Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Model Training</a></span></li><li><span><a href=\"#Prediction-and-Performance-Evaluation\" data-toc-modified-id=\"Prediction-and-Performance-Evaluation-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Prediction and Performance Evaluation</a></span></li></ul></li><li><span><a href=\"#Newer-Supervised-Deep-Learning-Models\" data-toc-modified-id=\"Newer-Supervised-Deep-Learning-Models-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Newer Supervised Deep Learning Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prediction-class-label-encoding\" data-toc-modified-id=\"Prediction-class-label-encoding-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Prediction class label encoding</a></span></li><li><span><a href=\"#Feature-Engineering-with-word-embeddings\" data-toc-modified-id=\"Feature-Engineering-with-word-embeddings-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Feature Engineering with word embeddings</a></span></li><li><span><a href=\"#Modeling-with-deep-neural-networks\" data-toc-modified-id=\"Modeling-with-deep-neural-networks-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Modeling with deep neural networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Building-Deep-neural-network-architecture\" data-toc-modified-id=\"Building-Deep-neural-network-architecture-4.3.3.1\"><span class=\"toc-item-num\">4.3.3.1&nbsp;&nbsp;</span>Building Deep neural network architecture</a></span></li><li><span><a href=\"#Model-Training,-Prediction-and-Performance-Evaluation\" data-toc-modified-id=\"Model-Training,-Prediction-and-Performance-Evaluation-4.3.3.2\"><span class=\"toc-item-num\">4.3.3.2&nbsp;&nbsp;</span>Model Training, Prediction and Performance Evaluation</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Advanced-Supervised-Deep-Learning-Models\" data-toc-modified-id=\"Advanced-Supervised-Deep-Learning-Models-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Advanced Supervised Deep Learning Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preparing-data\" data-toc-modified-id=\"Preparing-data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Preparing data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenize-train-&amp;-test-datasets\" data-toc-modified-id=\"Tokenize-train-&amp;-test-datasets-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Tokenize train &amp; test datasets</a></span></li><li><span><a href=\"#Build-Vocabulary-Mapping-(word-to-index)\" data-toc-modified-id=\"Build-Vocabulary-Mapping-(word-to-index)-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Build Vocabulary Mapping (word to index)</a></span></li><li><span><a href=\"#Encode-and-Pad-datasets-&amp;-Encode-prediction-class-labels\" data-toc-modified-id=\"Encode-and-Pad-datasets-&amp;-Encode-prediction-class-labels-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Encode and Pad datasets &amp; Encode prediction class labels</a></span></li></ul></li><li><span><a href=\"#Build-the-LSTM-Model-Architecture\" data-toc-modified-id=\"Build-the-LSTM-Model-Architecture-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Build the LSTM Model Architecture</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-model-architecture\" data-toc-modified-id=\"Visualize-model-architecture-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Visualize model architecture</a></span></li></ul></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Train the model</a></span></li><li><span><a href=\"#Predict-and-Evaluate-Model-Performance\" data-toc-modified-id=\"Predict-and-Evaluate-Model-Performance-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Predict and Evaluate Model Performance</a></span></li></ul></li><li><span><a href=\"#Analyzing-Sentiment-Causation\" data-toc-modified-id=\"Analyzing-Sentiment-Causation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analyzing Sentiment Causation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-Text-Classification-Pipeline-with-The-Best-Model\" data-toc-modified-id=\"Build-Text-Classification-Pipeline-with-The-Best-Model-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Build Text Classification Pipeline with The Best Model</a></span></li><li><span><a href=\"#Interpreting-Predictive-Models\" data-toc-modified-id=\"Interpreting-Predictive-Models-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Interpreting Predictive Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analyze-Model-Prediction-Probabilities\" data-toc-modified-id=\"Analyze-Model-Prediction-Probabilities-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Analyze Model Prediction Probabilities</a></span></li><li><span><a href=\"#Interpreting-Model-Decisions\" data-toc-modified-id=\"Interpreting-Model-Decisions-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Interpreting Model Decisions</a></span></li></ul></li><li><span><a href=\"#Analyzing-Topic-Models\" data-toc-modified-id=\"Analyzing-Topic-Models-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Analyzing Topic Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-features-from-positive-and-negative-reviews\" data-toc-modified-id=\"Extract-features-from-positive-and-negative-reviews-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;</span>Extract features from positive and negative reviews</a></span></li><li><span><a href=\"#Topic-Modeling-on-Reviews\" data-toc-modified-id=\"Topic-Modeling-on-Reviews-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;</span>Topic Modeling on Reviews</a></span></li><li><span><a href=\"#Visualize-topics-for-positive-reviews\" data-toc-modified-id=\"Visualize-topics-for-positive-reviews-6.3.3\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;</span>Visualize topics for positive reviews</a></span></li><li><span><a href=\"#Display-and-visualize-topics-for-negative-reviews\" data-toc-modified-id=\"Display-and-visualize-topics-for-negative-reviews-6.3.4\"><span class=\"toc-item-num\">6.3.4&nbsp;&nbsp;</span>Display and visualize topics for negative reviews</a></span></li></ul></li></ul></li></ul></div>"
    },
    {
      "metadata": {
        "_uuid": "e95ac4c6d6527080343b524f7a9626cf2c6e8c05"
      },
      "cell_type": "markdown",
      "source": "## Introduction\n\nThe problem at hand is sentiment analysis or opinion mining, where we want to analyze some textual documents and predict their sentiment or opinion based on the content of these documents.\n\nA text corpus consists of multiple text documents and each document can be as simple as a single sentence to a complete document with multiple paragraphs. Textual data, in spite of being highly unstructured, can be classified into two major types of documents:\n- ***Factual/objective documents***: typically depict some form of statements or facts with no specific feelings or emotion attached to them. \n- ***Subjective documents***: text that expresses feelings, moods, emotions, and opinions.\n\nTypically sentiment analysis seems to work best on subjective text, where people express opinions, feelings, and their mood. From a real-world industry standpoint, sentiment analysis is widely used to analyze corporate surveys, feedback surveys, social media data, and reviews for movies, places, commodities, and many more. The idea is to analyze and understand the reactions of people toward a specific entity and take insightful actions based on their sentiment.\n\n![image](https://www.kdnuggets.com/images/sentiment-fig-1-689.jpg)\n\n**Sentiment analysis** is also popularly known as **opinion analysis** or **opinion mining**. The key idea is to use techniques from text analytics, NLP, Machine Learning, and linguistics to extract important information or data points from unstructured text. This in turn can help us derive ***qualitative outputs*** like the overall sentiment being on a ***positive***, ***neutral***, or ***negative*** scale and ***quantitative outputs*** like the sentiment ***polarity***, ***subjectivity***, and ***objectivity*** proportions. \n\n**Sentiment polarity** is typically a numeric score that's assigned to both the positive and negative aspects of a text document based on subjective parameters like specific words and phrases expressing feelings and emotion. Neutral sentiment typically has 0 polarity since it does not express and specific sentiment, positive sentiment will have polarity > 0, and negative < 0. Of course, you can always change these thresholds based on the type of text you are dealing with.\n\n### How to classify Sentiment?\n![image](https://www.kdnuggets.com/images/sentiment-fig-2-532.jpg)\n__Machine Learning__:\n\nThis approach, employes a machine-learning technique and diverse features to construct a classifier that can identify text that expresses sentiment. Nowadays, deep-learning methods are popular because they fit on data learning representations.\n\n__Lexicon-Based__:\n\nThis method uses a variety of words annotated by polarity score, to decide the general assessment score of a given content. The strongest asset of this technique is that it does not require any training data, while its weakest point is that a large number of words and expressions are not included in sentiment lexicons.\n\n__Hybrid__:\n\nThe combination of machine learning and lexicon-based approaches to address Sentiment Analysis is called Hybrid. Though not commonly used, this method usually produces more promising results than the approaches mentioned above.\n"
    },
    {
      "metadata": {
        "_uuid": "60620c098d158c01abad9f0133432832c92cde10"
      },
      "cell_type": "markdown",
      "source": "## Preparing environment and data\n### Import and Setting Up Dependencies\n\nLet’s load the necessary dependencies and settings before getting started."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "684eda5d87532d11b200c40262f9a8eab7144f9a",
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport spacy\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\nfrom bs4 import BeautifulSoup\nimport unicodedata\n\nnlp = spacy.load('en', parse = False, tag=False, entity=False)\ntokenizer = ToktokTokenizer()\n\nimport datetime\nfrom datetime import timedelta\n \ndatetimeFormat = '%Y-%m-%d %H:%M:%S.%f'\n\nfrom sklearn.preprocessing import LabelEncoder, label_binarize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import NMF\nfrom sklearn.base import clone\n\nfrom scipy import interp\n\nfrom afinn import Afinn\nafn = Afinn(emoticons=True) \n\nimport nltk\nnltk.download('all', halt_on_error=False)\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport gensim\n\nfrom collections import Counter\n\nfrom IPython.display import SVG\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Activation, Dense, Embedding, Dropout, SpatialDropout1D, LSTM, Bidirectional\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.preprocessing import sequence\n\n#from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\nfrom lime.lime_text import LimeTextExplainer\n\nimport pyLDAvis\nimport pyLDAvis.sklearn\n\nnp.set_printoptions(precision=2, linewidth=80)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading collection 'all'\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n[nltk_data]    | Downloading package comtrans to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/dolch.zip.\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package floresta to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package nps_chat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package paradigms to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package ppattach to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package propbank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package state_union to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package verbnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n[nltk_data]    | Downloading package webtext to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package tagsets to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package mte_teip5 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping misc/perluniprops.zip.\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package porter_test to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n[nltk_data]    | Downloading package mwa_ppdb to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "eb9a2832f4a75eed136d01b525968a57dfdf5234"
      },
      "cell_type": "markdown",
      "source": "**Notes**: NLP libraries which will be used include spacy, nltk, and gensim. Do remember to check that your installed nltk version is at least >= 3.2.4, otherwise, the ToktokTokenizer class may not be present. If you want to use a lower nltk version for some reason, you can use any other tokenizer like the default word_tokenize() based on the TreebankWordTokenizer. The version for gensim should be at least 2.3.0 and for spacy, the version used was 1.9.0. We recommend using the latest version of spacy which was recently released (version 2.x) as this has fixed several bugs and added several improvements.\n\n### Text Pre-Processing and Normalization\n\nAn initial step in text and sentiment classification is pre-processing. A significant amount of techniques is applied to data in order to improvement of classification effectiveness. This enables standardization across a document corpus, which helps build meaningful features, to reduce dimensionality and reduce noise that can be introduced due to many factors like irrelevant symbols, special characters, XML and HTML tags, and so on.\n\nThe main components in our text normalization pipeline are:"
    },
    {
      "metadata": {
        "_uuid": "dbe19a042c3bfe4cc705a10ed0641f16bee02729"
      },
      "cell_type": "markdown",
      "source": "#### Cleaning Text - strip HTML\nOur text often contains unnecessary content like HTML tags, which do not add much value when analyzing sentiment. Hence we need to make sure we remove them before extracting features. The BeautifulSoup library does an excellent job in providing necessary functions for this. Our strip_html_tags(...) function enables in cleaning and stripping out HTML code."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad69ec0f9c932cfa40586e08ad1e30fd158dce2f"
      },
      "cell_type": "code",
      "source": "def strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9ada07fcff515a1b479456c628031b205ce6cbf1"
      },
      "cell_type": "markdown",
      "source": "#### Removing accented characters\nIn our dataset, we are dealing with reviews in the English language so we need to make sure that characters with any other format, especially accented characters are converted and standardized into ASCII characters. A simple example would be converting é to e. Our remove_accented_chars(...) function helps us in this respect."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a66111cd86afbf71afb381c07fd29aad2fb71510"
      },
      "cell_type": "code",
      "source": "def remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e028ca43e7fa37449e66dc91a07d1be695616cd9"
      },
      "cell_type": "markdown",
      "source": "#### Expanding Contractions\nIn the English language, contractions are basically shortened versions of words or syllables. Contractions pose a problem in text normalization because we have to deal with special characters like the apostrophe and we also have to convert each contraction to its expanded, original form. Our expand_contractions(...) function uses regular expressions and various contractions mapped to expand all contractions in our text corpus."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d17dd783d10ddb29b4bd2b92c14c6de55b461bb4"
      },
      "cell_type": "code",
      "source": "# -*- coding: utf-8 -*-\n\n# Contraction Map\nCONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ndef expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    \n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n                                      flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match)\\\n                                if contraction_mapping.get(match)\\\n                                else contraction_mapping.get(match.lower())                       \n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n        \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "32f01ef28221b2ac7c3292aaa7020241e6018cc9"
      },
      "cell_type": "markdown",
      "source": "#### Removing Special Characters\nSimple regexes can be used to achieve this. Our function remove_special_characters(...) helps us remove special characters. In our code, we have retained numbers but you can also remove numbers if you do not want them in your normalized corpus."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1e9733ea613c748ef985aceec6432e4d38b05e5"
      },
      "cell_type": "code",
      "source": "def remove_special_characters(text):\n    text = re.sub(r'[^a-zA-z0-9\\s]', '', text)\n    return text",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d615e8f3f7ae86d29dc0511e81c7aef6135cb072"
      },
      "cell_type": "markdown",
      "source": "#### Lemmatizing text\n**Word stems** are usually the base form of possible words that can be created by ***attaching affixes*** like prefixes and suffixes ***to the stem*** to create new words. This is known as **inflection**. The **reverse process** of obtaining the base form of a word is known as **stemming**. The nltk package offers a wide range of stemmers like the PorterStemmer and LancasterStemmer. **Lemmatization** is very similar to stemming, where we remove word affixes to get to the base form of a word. However the base form in this case is known as the **root word** but not the root stem. The difference being that ***the root word is always a lexicographically correct word***, present in the dictionary, but the root stem may not be so. We will be using lemmatization only in our normalization pipeline to retain lexicographically correct words. The function lemmatize_text(...) helps us with this aspect."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17730ba345b559e93ba0c54197505f57114a20e0"
      },
      "cell_type": "code",
      "source": "def lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd2e6a21537291231317c521eb7eeaff4ccbb52b"
      },
      "cell_type": "markdown",
      "source": "#### Removing Stopwords\nWords which have little or no significance especially when constructing meaningful features from text are also known as stopwords or stop words. These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a document corpus. Words like a, an, the, and so on are considered to be stopwords. There is no universal stopword list but we use a standard English language stopwords list from nltk. You can also add your own domain specific stopwords if needed. The function remove_stopwords(...) helps us remove stopwords and retain words having the most significance and context in a corpus."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5e0e1bec9a1e48407d3f6ee89336e6de577073a"
      },
      "cell_type": "code",
      "source": "stopword_list = nltk.corpus.stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')\n\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f2d964cd0258402be87ddb2b623273cc89c4696"
      },
      "cell_type": "markdown",
      "source": "#### Normalize text corpus - tying it all together\n\nWe use all these components and tie them together in the following function called normalize_corpus(...), which can be used to take a document corpus as input and return the same corpus with cleaned and normalized text documents."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b502d950565af91ded35353125d0e805c279bdc"
      },
      "cell_type": "code",
      "source": "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n                     accented_char_removal=True, text_lower_case=True, \n                     text_lemmatization=True, special_char_removal=True, \n                     stopword_removal=True):\n    \n    normalized_corpus = []\n    # normalize each document in the corpus\n    for doc in corpus:\n        # strip HTML\n        if html_stripping:\n            doc = strip_html_tags(doc)\n        # remove accented characters\n        if accented_char_removal:\n            doc = remove_accented_chars(doc)\n        # expand contractions    \n        if contraction_expansion:\n            doc = expand_contractions(doc)\n        # lowercase the text    \n        if text_lower_case:\n            doc = doc.lower()\n        # remove extra newlines\n        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n        # insert spaces between special characters to isolate them    \n        special_char_pattern = re.compile(r'([{.(-)!}])')\n        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n        # lemmatize text\n        if text_lemmatization:\n            doc = lemmatize_text(doc)\n        # remove special characters    \n        if special_char_removal:\n            doc = remove_special_characters(doc)  \n        # remove extra whitespace\n        doc = re.sub(' +', ' ', doc)\n        # remove stopwords\n        if stopword_removal:\n            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n            \n        normalized_corpus.append(doc)\n        \n    return normalized_corpus\n",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bc6f849db45552bcc4e4ef9870a8c0c92d8cd53a"
      },
      "cell_type": "markdown",
      "source": "### Topics Help Functions\n\nWe will also leverage some utility functions to support get and display topics from a corpus with their terms and weights."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aeeeab503fffd2e6edaaa428b5e0d8423c6079a8"
      },
      "cell_type": "code",
      "source": "# Prints components of all the topics obtained from topic modeling\ndef print_topics_udf(topics, total_topics=1,\n                     weight_threshold=0.0001,\n                     display_weights=False,\n                     num_terms=None):\n    \n    for index in range(total_topics):\n        topic = topics[index]\n        topic = [(term, float(wt))\n                 for term, wt in topic]\n        topic = [(word, round(wt,2)) \n                 for word, wt in topic \n                 if abs(wt) >= weight_threshold]\n                     \n        if display_weights:\n            print('Topic #'+str(index+1)+' with weights')\n            print(topic[:num_terms]) if num_terms else topic\n        else:\n            print('Topic #'+str(index+1)+' without weights')\n            tw = [term for term, wt in topic]\n            print(tw[:num_terms]) if num_terms else tw\n        print()\n        \n\n# Extracts topics with their terms and weights \n# Format is Topic N: [(term1, weight1), ..., (termn, weightn)]        \ndef get_topics_terms_weights(weights, feature_names):\n    feature_names = np.array(feature_names)\n    sorted_indices = np.array([list(row[::-1]) \n                           for row \n                           in np.argsort(np.abs(weights))])\n    sorted_weights = np.array([list(wt[index]) \n                               for wt, index \n                               in zip(weights,sorted_indices)])\n    sorted_terms = np.array([list(feature_names[row]) \n                             for row \n                             in sorted_indices])\n    \n    topics = [np.vstack((terms.T, term_weights.T)).T \n              for terms, term_weights \n              in zip(sorted_terms, sorted_weights)]     \n    \n    return topics         ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d642b10d39068d6350e5dfdccd7a41b72b4143fa"
      },
      "cell_type": "markdown",
      "source": "### Simplify Get Results\nLet's build a function to standardize the capture and exposure of the results of our models.\n\nAs a classification problem, Sentiment Analysis uses the evaluation metrics of Precision, Recall, F-score, and Accuracy. Also, average measures like macro, micro, and weighted F1-scores are useful for multi-class problems. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "390d2827bfb7c7a0c9ab88d831db490d9a90018a"
      },
      "cell_type": "code",
      "source": "def get_results(model, name, data, true_labels, target_names = ['positive', 'negative'], results=None, reasume=False):\n\n    if hasattr(model, 'layers'):\n        param = wtp_dnn_model.history.params\n        best = np.mean(wtp_dnn_model.history.history['val_acc'])\n        predicted_labels = model.predict_classes(data) \n        im_model = InMemoryModel(model.predict, examples=data, target_names=target_names)\n\n    else:\n        param = gs.best_params_\n        best = gs.best_score_\n        predicted_labels = model.predict(data).ravel()\n        if hasattr(model, 'predict_proba'):\n            im_model = InMemoryModel(model.predict_proba, examples=data, target_names=target_names)\n        elif hasattr(clf, 'decision_function'):\n            im_model = InMemoryModel(model.decision_function, examples=data, target_names=target_names)\n        \n    print('Mean Best Accuracy: {:2.2%}'.format(best))\n    print('-'*60)\n    print('Best Parameters:')\n    print(param)\n    print('-'*60)\n    \n    y_pred = model.predict(data).ravel()\n    \n    display_model_performance_metrics(true_labels, predicted_labels = predicted_labels, target_names = target_names)\n    if len(target_names)==2:\n        ras = roc_auc_score(y_true=true_labels, y_score=y_pred)\n    else:\n        roc_auc_multiclass, ras = roc_auc_score_multiclass(y_true=true_labels, y_score=y_pred, target_names=target_names)\n        print('\\nROC AUC Score by Classes:\\n',roc_auc_multiclass)\n        print('-'*60)\n\n    print('\\n\\n              ROC AUC Score: {:2.2%}'.format(ras))\n    prob, score_roc, roc_auc = plot_model_roc_curve(model, data, true_labels, label_encoder=None, class_names=target_names)\n    \n    interpreter = Interpretation(data, feature_names=cols)\n    plots = interpreter.feature_importance.plot_feature_importance(im_model, progressbar=False, n_jobs=1, ascending=True)\n    \n    r1 = pd.DataFrame([(prob, best, np.round(accuracy_score(true_labels, predicted_labels), 4), \n                         ras, roc_auc)], index = [name],\n                         columns = ['Prob', 'CV Accuracy', 'Accuracy', 'ROC AUC Score', 'ROC Area'])\n    if reasume:\n        results = r1\n    elif (name in results.index):        \n        results.loc[[name], :] = r1\n    else: \n        results = results.append(r1)\n        \n    return results\n\ndef roc_auc_score_multiclass(y_true, y_score, target_names, average = \"macro\"):\n\n  #creating a set of all the unique classes using the actual class list\n  unique_class = set(y_true)\n  roc_auc_dict = {}\n  mean_roc_auc = 0\n  for per_class in unique_class:\n    #creating a list of all the classes except the current class \n    other_class = [x for x in unique_class if x != per_class]\n\n    #marking the current class as 1 and all other classes as 0\n    new_y_true = [0 if x in other_class else 1 for x in y_true]\n    new_y_score = [0 if x in other_class else 1 for x in y_score]\n    num_new_y_true = sum(new_y_true)\n\n    #using the sklearn metrics method to calculate the roc_auc_score\n    roc_auc = roc_auc_score(new_y_true, new_y_score, average = average)\n    roc_auc_dict[target_names[per_class]] = np.round(roc_auc, 4)\n    mean_roc_auc += num_new_y_true * np.round(roc_auc, 4)\n    \n  mean_roc_auc = mean_roc_auc/len(y_true)  \n  return roc_auc_dict, mean_roc_auc\n\ndef get_metrics(true_labels, predicted_labels):\n    \n    print('Accuracy:  {:2.2%} '.format(metrics.accuracy_score(true_labels, predicted_labels)))\n    print('Precision: {:2.2%} '.format(metrics.precision_score(true_labels, predicted_labels, average='weighted')))\n    print('Recall:    {:2.2%} '.format(metrics.recall_score(true_labels, predicted_labels, average='weighted')))\n    print('F1 Score:  {:2.2%} '.format(metrics.f1_score(true_labels, predicted_labels, average='weighted')))\n                        \n\ndef train_predict_model(classifier,  train_features, train_labels,  test_features, test_labels):\n    # build model    \n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features) \n    return predictions    \n\n\ndef display_confusion_matrix(true_labels, predicted_labels, target_names):\n    \n    total_classes = len(target_names)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[['Predicted:'], target_names], labels=level_labels), \n                            index=pd.MultiIndex(levels=[['Actual:'], target_names], labels=level_labels)) \n    print(cm_frame) \n    \ndef display_classification_report(true_labels, predicted_labels, target_names):\n\n    report = metrics.classification_report(y_true=true_labels, y_pred=predicted_labels, target_names=target_names) \n    print(report)\n    \ndef display_model_performance_metrics(true_labels, predicted_labels, target_names):\n    print('Model Performance metrics:')\n    print('-'*30)\n    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n    print('\\nModel Classification report:')\n    print('-'*30)\n    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, target_names=target_names)\n    print('\\nPrediction Confusion Matrix:')\n    print('-'*30)\n    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, target_names=target_names)\n\n\ndef plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n    \n    ## Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    if hasattr(clf, 'classes_'):\n        class_labels = clf.classes_\n    elif label_encoder:\n        class_labels = label_encoder.classes_\n    elif class_names:\n        class_labels = class_names\n    else:\n        raise ValueError('Unable to derive prediction classes, please specify class_names!')\n    n_classes = len(class_labels)\n   \n    if n_classes == 2:\n        if hasattr(clf, 'predict_proba'):\n            prb = clf.predict_proba(features)\n            if prb.shape[1] > 1:\n                y_score = prb[:, prb.shape[1]-1] \n            else:\n                y_score = clf.predict(features).ravel()\n            prob = True\n        elif hasattr(clf, 'decision_function'):\n            y_score = clf.decision_function(features)\n            prob = False\n        else:\n            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n        \n        fpr, tpr, _ = roc_curve(true_labels, y_score)      \n        roc_auc = auc(fpr, tpr)\n\n        plt.plot(fpr, tpr, label='ROC curve (area = {0:3.2%})'.format(roc_auc), linewidth=2.5)\n        \n    elif n_classes > 2:\n        if  hasattr(clf, 'clfs_'):\n            y_labels = label_binarize(true_labels, classes=list(range(len(class_labels))))\n        else:\n            y_labels = label_binarize(true_labels, classes=class_labels)\n        if hasattr(clf, 'predict_proba'):\n            y_score = clf.predict_proba(features)\n            prob = True\n        elif hasattr(clf, 'decision_function'):\n            y_score = clf.decision_function(features)\n            prob = False\n        else:\n            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n            \n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_labels[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        ## Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_labels.ravel(), y_score.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n        ## Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n        # Finally average it and compute AUC\n        mean_tpr /= n_classes\n        fpr[\"macro\"] = all_fpr\n        tpr[\"macro\"] = mean_tpr\n        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n        ## Plot ROC curves\n        plt.figure(figsize=(6, 4))\n        plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:2.2%})'\n                       ''.format(roc_auc[\"micro\"]), linewidth=3)\n\n        plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:2.2%})'\n                       ''.format(roc_auc[\"macro\"]), linewidth=3)\n\n        for i, label in enumerate(class_names):\n            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:2.2%})'\n                                           ''.format(label, roc_auc[i]), linewidth=2, linestyle=':')\n        roc_auc = roc_auc[\"macro\"]   \n    else:\n        raise ValueError('Number of classes should be atleast 2 or more')\n        \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.01, 1.0])\n    plt.ylim([0.0, 1.01])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return prob, y_score, roc_auc",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "736d541e760a6e5061cf9a6d45497022ad18db62"
      },
      "cell_type": "markdown",
      "source": "### Load and normalize data\nWe can now load our IMDb movie reviews dataset, use the first 40,000 reviews for training models and the remaining 10,000 reviews as the test dataset to evaluate model performance."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "df68c4b6691ddc95ce72af60d66afda8aa52f2b6"
      },
      "cell_type": "code",
      "source": "dataset = pd.read_csv(r'../input/movie_reviews.csv')\nreviews = np.array(dataset['review'])\nsentiments = np.array(dataset['sentiment'])\n\n# take a peek at the data\ndisplay(dataset.head())\n\n# build train and test datasets\ntrain_reviews, test_reviews, train_sentiments, test_sentiments =\\\n    train_test_split(reviews, sentiments , test_size=0.20,  random_state=101)\n\nsample_review_ids = [7626, 3533, 9010]",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "34355b3ab36bf4ef1ae4fa7abf3624a278c2fee2"
      },
      "cell_type": "markdown",
      "source": " Now, we will also use our normalization module to normalize our review datasets. This is a time-consuming operation."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79208bdaa098b439e29936c4bd22e9a2451660a1"
      },
      "cell_type": "code",
      "source": "now = datetime.datetime.now()\nprint('Current date and time: {}'.format(now.strftime(\"%Y-%m-%d %H:%M:%S\")))\n\n# normalize training dataset\nprint('-'*60)\nprint('Normalize training dataset:')\nnorm_train_reviews = normalize_corpus(train_reviews)\ndiff = (datetime.datetime.now() - now)\nnow = datetime.datetime.now()\nprint('Elapsed time: {}\\n'.format(diff))\n\n# normalize test dataset\nprint('-'*60)\nprint('Normalize test dataset:')\nnorm_test_reviews = normalize_corpus(test_reviews)\ndiff = (datetime.datetime.now() - now)\nprint('Elapsed time: {}\\n'.format(diff))",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Current date and time: 2019-01-30 15:45:15\n------------------------------------------------------------\nNormalize training dataset:\nElapsed time: 0:42:41.881894\n\n------------------------------------------------------------\nNormalize test dataset:\nElapsed time: 0:10:23.421805\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "38ac98f20112167bd7e1ab062d4042e46d165acc"
      },
      "cell_type": "markdown",
      "source": "## Sentiment Analysis - Unsupervised Lexical\n\nEven though we have labeled data, this section should give you a good idea of how lexicon based models work and you can apply the same in your own datasets when you do not have labeled data.\n\nUnsupervised sentiment analysis models use well curated knowledgebases, ontologies, lexicons, and databases that have detailed information pertaining to subjective words, phrases including sentiment, mood, polarity, objectivity, subjectivity, and so on. A lexicon model typically uses a lexicon, also known as a dictionary or vocabulary of words specifically aligned toward sentiment analysis. Usually these lexicons contain a list of words associated with positive and negative sentiment, polarity (magnitude of negative or positive score), parts of speech (POS) tags, subjectivity classifiers (strong, weak, neutral), mood, modality, and so on. You can use these lexicons and compute sentiment of a text document by matching the presence of specific words from the lexicon, look at other additional factors like presence of negation parameters, surrounding words, overall context and phrases and aggregate overall sentiment polarity scores to decide the final sentiment score. \n\n![image](https://image.slidesharecdn.com/iccbr-12-main-121111094448-phpapp01/95/sentiment-classification-with-casebased-reasoning-10-638.jpg)\n\nThere are several popular lexicon models used for sentiment analysis. Some of them are mentioned as follows.\n- Bing Liu’s Lexicon\n- MPQA Subjectivity Lexicon\n- Pattern Lexicon\n- AFINN Lexicon\n- SentiWordNet Lexicon\n- VADER Lexicon\n\nThis is not an exhaustive list of lexicon models, but definitely lists among the most popular ones available today. Since we have labeled data, it will be easy for us to see how well our actual sentiment values for these movie reviews match our lexiconmodel based predicted sentiment values. We will be covering the last three lexicon models in more detail and predict their sentiment and see how well our model performs based on model evaluation metrics like accuracy, precision, recall, and F1-score.\n\n\n### Sentiment Analysis with AFINN\n\nThe [AFINN lexicon](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) is perhaps one of the simplest and most popular lexicons that can be used extensively for sentiment analysis. It is a list of words rated for valence with an integer between minus five (negative) and plus five (positive).  The current version of the lexicon is [AFINN-en-165.txt](https://github.com/fnielsen/afinn/blob/master/afinn/data/) and it contains over 3,300+ words with a polarity score associated with each word. The author has also created a nice wrapper library on top of this in Python called afinn which we will be using for our analysis needs. AFINN takes into account other aspects like emoticons and exclamations.\n![image](https://image.slidesharecdn.com/phpbnl18-machine-learning-180126163450/95/learning-machine-learning-31-638.jpg)\n\nWe can now use this object and compute the polarity of our chosen four sample reviews. The results permit you compare the actual sentiment label for each review and also check out the predicted sentiment polarity score. A negative polarity typically denotes negative sentiment. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20dd660b16893f8b7da5964d654f13f3ab459f78"
      },
      "cell_type": "code",
      "source": "sample_review_ids = [7626, 3533, 9010]",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb1e24c1e6cb034bd1fe03d703e4a383fc1f05c2"
      },
      "cell_type": "code",
      "source": "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n    print('REVIEW:', review)\n    print('Actual Sentiment:', sentiment)\n    print('Predicted Sentiment polarity:', afn.score(review))\n    print('-'*60)\n    print('\\n')",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "REVIEW: For those not in the know, the Asterix books are a hugely successful series of comic books about a village of indomitable Gauls who resist Caesar's invasion thanks to a magic potion that renders them invulnerable supermen. There have been several animated features (only one of them, The Twelve Tasks of Asterix really capturing the wit and spirit of the books despite being an original screen story) before a perfectly cast Christian Clavier and Gerard Depardieu took the lead roles in two live action adaptations that proved colossally successful throughout Europe but made no impression whatsoever in the English-speaking world. <br /><br />The uncut French version is great fun, but sadly does not appear to be available in a version with English subtitles outside of the UK DVD. While there's still no sign of a US theatrical or DVD release, the Miramax version of Asterix et Obelix: Mission Cleopatre is also on that DVD (and has played on UK TV), and you'll never guess what - it's been completely re-edited (at least 21 minutes gone) and dubbed into English. Maybe Harve mistook it for a Hong Kong movie - after all, he never saw a foreign film he didn't think couldn't be improved by heavy re-editing and shelving for a few years.<br /><br />Whereas Asterix et Obelix Contre Cesar was lovingly dubbed into English from a particularly good translation script by Terry Jones but otherwise left unaltered, that sort of thing really isn't the Miramax way. The results ain't good. The film was the best attempt to get the books mixture of slapstick, anachronisms and highbrow classical humorous asides to the screen, but a lot of the classical references are gone (such as the great Raft of the Medusa sight gag or the Cyrano de Bergerac references from Depardieu), alongside anything that seems too French or might slow the picture down, with the result that the first 20 minutes are now a real slog. Several punchlines to sequences are missing, Depardieu's part has been trimmed (his part was already fairly small because of his serious health problems during the shoot: the US version has been partially digitally regraded to change the unhealthy pallor of his face in the original!), and as usual with dubbing, because literal translations into English don't fit properly, lines are either rushed so much they're not funny anymore or the dialogue has been changed completely (a couple of these changes are admittedly funny, like one character dreaming of a world in which he could move his lips in French and hear the words in English).<br /><br />Not a total disaster, but very disappointing considering how good the full-length version is. It would be nice to think that Miramax would do a Shaolin Soccer and release both versions, but since they've shelved both films for two years since paying $45m for them (another classic case of Harvey's notorious chronic buyer's remorse: gee, wonder why Disney were so p****d at their overspending) and still have no release plans, that may just be too much wishful thinking.<br /><br />It's a real pity that such an accessible and entertaining film will now only be available to non-French speakers in such a clumsily bowdlerised version. It seems the plucky Gauls may have been able to defeat Caesar's legions but are no match for the Miramax jackboot.\nActual Sentiment: positive\nPredicted Sentiment polarity: 30.0\n--------------------------------------------------------------------------------\n\n\nREVIEW: Horrendously acted and completely laughable haunted-house horror flick that has an out of place Anna Paquin playing a neurotic teenager fighting off the \"things-that-go-bump-in-the-dark\" that are plaguing her and her family shortly after moving to their new home in Spain(?!). Little more than a geographically re-planted rip-off of \"The Shining\" and most notably \"The Others\", the weak-plotted \"Darkness\" is basically your typical run-of-the mill B-horror feature with a few predictable lame scares that can be seen by audiences a mile off (so to speak)! In retrospect I suppose I shouldn't have set my personal expectations quite as high for this movie to actually be good considering the well-known fact that it was shelved for nearly three years before finally being released around Christmas of last year in American cinemas across the country to what was ultimately lukewarm ticket-sales and very harsh reviews from critics. When will filmmakers ever learn that there's more to making movies (be it horror or otherwise) than just the fey possibility of a little financial gain? (Turkey-Zero Stars)\nActual Sentiment: negative\nPredicted Sentiment polarity: -11.0\n--------------------------------------------------------------------------------\n\n\nREVIEW: This is the only David Zucker movie that does not spoof anything the first of its kind. The funniest movie of 98 with Night at the Roxbury right behind But I did not think Theres something about mary was funny so that doesnt count except for the frank and beans thing he he. Dont listen to the critics especially Roger Ebert he does not know solid entertainment just look at his reviews.Anyway see it you wont be dissapionted\nActual Sentiment: positive\nPredicted Sentiment polarity: 6.0\n--------------------------------------------------------------------------------\n\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "aee8493c3b4cdb7f8bbf1349ac5f6efd5282e777"
      },
      "cell_type": "markdown",
      "source": "Below we used a threshold of >= 2.0 to determine if the overall sentiment is positive else negative. You can choose your own threshold based on analyzing your own corpora in the future."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4062a7c125450ff6b00aa8d3ee8b5e56ce4413d4"
      },
      "cell_type": "code",
      "source": "sentiment_polarity = [afn.score(review) for review in test_reviews]\npredicted_sentiments = ['positive' if score >= 2.0 else 'negative' for score in sentiment_polarity]",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "05685f440efe5338cf66c7d6ff75f133fe087cb8"
      },
      "cell_type": "markdown",
      "source": "Now that we have our predicted sentiment labels, we can evaluate our model performance based on standard performance metrics using our utility function."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03ac7d475aa593b64c003c084034091c710c397b"
      },
      "cell_type": "code",
      "source": "display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n                                  target_names=['positive', 'negative'])",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Model Performance metrics:\n------------------------------\nAccuracy:  72.42% \nPrecision: 73.20% \nRecall:    72.42% \nF1 Score:  72.15% \n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n    positive       0.77      0.63      0.69      4959\n    negative       0.69      0.82      0.75      5041\n\n   micro avg       0.72      0.72      0.72     10000\n   macro avg       0.73      0.72      0.72     10000\nweighted avg       0.73      0.72      0.72     10000\n\n\nPrediction Confusion Matrix:\n------------------------------\n                 Predicted:         \n                   positive negative\nActual: positive       3111     1848\n        negative        910     4131\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "aa051af5b3157f5dcb698578c3887155465443fa"
      },
      "cell_type": "markdown",
      "source": "We get an overall F1-Score of 72%, which is quite decent considering it's an unsupervised model. Looking at the confusion matrix we can clearly see that quite a number of positive sentiment based reviews have been misclassified as negative (1,848) and this leads to the lower recall of 63% for the positive sentiment class. Performance for negative class is better with regard to recall or f1-score, where we correctly predicted 4,131 out of 5,041 negative reviews, but precision is 69% because of the many wrong negative predictions made in case of positive sentiment reviews.\n\n### Sentiment Analysis with SentiWordNet\nThe WordNet corpus is definitely one of the most popular corpora for the English language used extensively in natural language processing and semantic analysis. WordNet gave us the concept of ***synsets*** or ***synonym sets***. The SentiWordNet lexicon is based on WordNet synsets and can be used for sentiment analysis and opinion mining. The [SentiWordNet](http://sentiwordnet.isti.cnr.it) lexicon typically assigns three sentiment scores for each WordNet synset. These include a positive polarity score, a negative polarity score and an objectivity score. We will be using the nltk library, which provides a Pythonic interface into [SentiWordNet](https://pt.coursera.org/lecture/text-mining-analytics/5-6-how-to-do-sentiment-analysis-with-sentiwordnet-5RwtX). Consider we have the adjective awesome. \n![image](https://player.slideplayer.com/11/3238511/data/images/img18.png)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ccda1714c5705fdf6bf8e90ada754f6f7dbe6a0"
      },
      "cell_type": "code",
      "source": "awesome = list(swn.senti_synsets('awesome', 'a'))[0]\nprint('Positive Polarity Score:', awesome.pos_score())\nprint('Negative Polarity Score:', awesome.neg_score())\nprint('Objective Score:', awesome.obj_score())",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Positive Polarity Score: 0.875\nNegative Polarity Score: 0.125\nObjective Score: 0.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1916fc9b018866443601c9447e07fde2c1de6784"
      },
      "cell_type": "markdown",
      "source": "Let's now build a generic function to extract and aggregate sentiment scores for a complete textual document based on matched synsets in that document. Our function basically takes in a movie review, tags each word with its corresponding POS tag, extracts out sentiment scores for any matched synset token based on its POS tag, and finally aggregates the scores. We can clearly see the predicted sentiment along with sentiment polarity scores and an objectivity score for each sample movie review depicted in formatted dataframes. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e208955ee7ec33cbb68e96cc1bb94918bf42be8"
      },
      "cell_type": "code",
      "source": "def analyze_sentiment_sentiwordnet_lexicon(review, verbose=False):\n\n    # tokenize and POS tag text tokens\n    tagged_text = [(token.text, token.tag_) for token in nlp(review)]\n    pos_score = neg_score = token_count = obj_score = 0\n    # get wordnet synsets based on POS tags\n    # get sentiment scores if synsets are found\n    for word, tag in tagged_text:\n        ss_set = None\n        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n        # if senti-synset is found        \n        if ss_set:\n            # add scores for all found synsets\n            pos_score += ss_set.pos_score()\n            neg_score += ss_set.neg_score()\n            obj_score += ss_set.obj_score()\n            token_count += 1\n    \n    # aggregate final scores\n    final_score = pos_score - neg_score\n    norm_final_score = round(float(final_score) / token_count, 2)\n    final_sentiment = 'positive' if norm_final_score >= 0.05 else 'negative'\n    if verbose:\n        norm_obj_score = round(float(obj_score) / token_count, 2)\n        norm_pos_score = round(float(pos_score) / token_count, 2)\n        norm_neg_score = round(float(neg_score) / token_count, 2)\n        # to display results in a nice table\n        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, \n                                         norm_neg_score, norm_final_score]],\n                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n                                                             ['Predicted Sentiment', 'Objectivity',\n                                                              'Positive', 'Negative', 'Overall']], \n                                                             labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n        display(sentiment_frame)\n        \n    return final_sentiment",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ffcd32dc8350bd139d9367f2b010e2f318de7a90"
      },
      "cell_type": "markdown",
      "source": "Let's use this model now to predict the sentiment of samples reviews and compare their results with its actual values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a8234af4c33e48fed3fd3eb4c654d2655af218b"
      },
      "cell_type": "code",
      "source": "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n    print('REVIEW:\\n', review)\n    print('\\nActual Sentiment:', sentiment)\n    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)    ",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "REVIEW:\n For those not in the know, the Asterix books are a hugely successful series of comic books about a village of indomitable Gauls who resist Caesar's invasion thanks to a magic potion that renders them invulnerable supermen. There have been several animated features (only one of them, The Twelve Tasks of Asterix really capturing the wit and spirit of the books despite being an original screen story) before a perfectly cast Christian Clavier and Gerard Depardieu took the lead roles in two live action adaptations that proved colossally successful throughout Europe but made no impression whatsoever in the English-speaking world. <br /><br />The uncut French version is great fun, but sadly does not appear to be available in a version with English subtitles outside of the UK DVD. While there's still no sign of a US theatrical or DVD release, the Miramax version of Asterix et Obelix: Mission Cleopatre is also on that DVD (and has played on UK TV), and you'll never guess what - it's been completely re-edited (at least 21 minutes gone) and dubbed into English. Maybe Harve mistook it for a Hong Kong movie - after all, he never saw a foreign film he didn't think couldn't be improved by heavy re-editing and shelving for a few years.<br /><br />Whereas Asterix et Obelix Contre Cesar was lovingly dubbed into English from a particularly good translation script by Terry Jones but otherwise left unaltered, that sort of thing really isn't the Miramax way. The results ain't good. The film was the best attempt to get the books mixture of slapstick, anachronisms and highbrow classical humorous asides to the screen, but a lot of the classical references are gone (such as the great Raft of the Medusa sight gag or the Cyrano de Bergerac references from Depardieu), alongside anything that seems too French or might slow the picture down, with the result that the first 20 minutes are now a real slog. Several punchlines to sequences are missing, Depardieu's part has been trimmed (his part was already fairly small because of his serious health problems during the shoot: the US version has been partially digitally regraded to change the unhealthy pallor of his face in the original!), and as usual with dubbing, because literal translations into English don't fit properly, lines are either rushed so much they're not funny anymore or the dialogue has been changed completely (a couple of these changes are admittedly funny, like one character dreaming of a world in which he could move his lips in French and hear the words in English).<br /><br />Not a total disaster, but very disappointing considering how good the full-length version is. It would be nice to think that Miramax would do a Shaolin Soccer and release both versions, but since they've shelved both films for two years since paying $45m for them (another classic case of Harvey's notorious chronic buyer's remorse: gee, wonder why Disney were so p****d at their overspending) and still have no release plans, that may just be too much wishful thinking.<br /><br />It's a real pity that such an accessible and entertaining film will now only be available to non-French speakers in such a clumsily bowdlerised version. It seems the plucky Gauls may have been able to defeat Caesar's legions but are no match for the Miramax jackboot.\n\nActual Sentiment: positive\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "     SENTIMENT STATS:                                      \n  Predicted Sentiment Objectivity Positive Negative Overall\n0            negative        0.84      0.1     0.06    0.04",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"5\" halign=\"left\">SENTIMENT STATS:</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Predicted Sentiment</th>\n      <th>Objectivity</th>\n      <th>Positive</th>\n      <th>Negative</th>\n      <th>Overall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>0.84</td>\n      <td>0.1</td>\n      <td>0.06</td>\n      <td>0.04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "REVIEW:\n Horrendously acted and completely laughable haunted-house horror flick that has an out of place Anna Paquin playing a neurotic teenager fighting off the \"things-that-go-bump-in-the-dark\" that are plaguing her and her family shortly after moving to their new home in Spain(?!). Little more than a geographically re-planted rip-off of \"The Shining\" and most notably \"The Others\", the weak-plotted \"Darkness\" is basically your typical run-of-the mill B-horror feature with a few predictable lame scares that can be seen by audiences a mile off (so to speak)! In retrospect I suppose I shouldn't have set my personal expectations quite as high for this movie to actually be good considering the well-known fact that it was shelved for nearly three years before finally being released around Christmas of last year in American cinemas across the country to what was ultimately lukewarm ticket-sales and very harsh reviews from critics. When will filmmakers ever learn that there's more to making movies (be it horror or otherwise) than just the fey possibility of a little financial gain? (Turkey-Zero Stars)\n\nActual Sentiment: negative\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "     SENTIMENT STATS:                                      \n  Predicted Sentiment Objectivity Positive Negative Overall\n0            negative        0.84     0.09     0.07    0.02",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"5\" halign=\"left\">SENTIMENT STATS:</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Predicted Sentiment</th>\n      <th>Objectivity</th>\n      <th>Positive</th>\n      <th>Negative</th>\n      <th>Overall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>0.84</td>\n      <td>0.09</td>\n      <td>0.07</td>\n      <td>0.02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "REVIEW:\n This is the only David Zucker movie that does not spoof anything the first of its kind. The funniest movie of 98 with Night at the Roxbury right behind But I did not think Theres something about mary was funny so that doesnt count except for the frank and beans thing he he. Dont listen to the critics especially Roger Ebert he does not know solid entertainment just look at his reviews.Anyway see it you wont be dissapionted\n\nActual Sentiment: positive\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "     SENTIMENT STATS:                                      \n  Predicted Sentiment Objectivity Positive Negative Overall\n0            negative        0.85     0.08     0.07    0.01",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"5\" halign=\"left\">SENTIMENT STATS:</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Predicted Sentiment</th>\n      <th>Objectivity</th>\n      <th>Positive</th>\n      <th>Negative</th>\n      <th>Overall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>0.85</td>\n      <td>0.08</td>\n      <td>0.07</td>\n      <td>0.01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ae38f6f0ac1363beb79de84c45b516011636a363"
      },
      "cell_type": "markdown",
      "source": "Let's use this model now to predict the sentiment of all our test reviews and evaluate its performance. A threshold of >=0 has been used for the overall sentiment polarity to be classified as positive and < 0 for negative sentiment."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d54fa4f65e4a972364a6a73446ddb85e9d37396f"
      },
      "cell_type": "code",
      "source": "predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in norm_test_reviews]\n\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n                                  target_names=['positive', 'negative'])",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Model Performance metrics:\n------------------------------\nAccuracy:  60.03% \nPrecision: 70.24% \nRecall:    60.03% \nF1 Score:  54.51% \n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n    positive       0.56      0.95      0.70      4959\n    negative       0.85      0.25      0.39      5041\n\n   micro avg       0.60      0.60      0.60     10000\n   macro avg       0.70      0.60      0.55     10000\nweighted avg       0.70      0.60      0.55     10000\n\n\nPrediction Confusion Matrix:\n------------------------------\n                 Predicted:         \n                   positive negative\nActual: positive       4726      233\n        negative       3764     1277\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "3ba4124a960bbe671e0e1421e0040332af5fbf5d"
      },
      "cell_type": "markdown",
      "source": "We get an overall F1-Score of 55%, which is definitely a step down from our AFINN based model. While we have lesser number of negative sentiment based reviews being misclassified as positive, the other aspects of the model performance have been affected.\n\n### Sentiment Analysis with VADER\nThe [VADER lexicon](https://www.researchgate.net/publication/275828927_VADER_A_Parsimonious_Rule-based_Model_for_Sentiment_Analysis_of_Social_Media_Text), developed by C.J. Hutto, is a lexicon that is based on a rule-based sentiment analysis framework, specifically tuned to analyze sentiments in social media. VADER stands for Valence Aware Dictionary and Sentiment Reasoner. You can use the library based on nltk's interface under the nltk.sentiment.vader module. Besides this, you can also [download the actual lexicon or install the framework](https://github.com/cjhutto/\nvaderSentiment). The file titled vader_lexicon.txt contains necessary sentiment scores associated with words, emoticons and slangs (like wtf, lol, nah, and so on). There were a total of over 9,000 lexical features from which over 7,500 curated lexical features were finally selected in the lexicon with proper validated valence scores. Each feature was rated on a scale from \"[-4] Extremely Negative\" to \"[4] Extremely Positive\", with allowance for \"[0] Neutral (or Neither, N/A)\". The process of selecting lexical features was done by keeping all features that had a non-zero mean rating and whose standard deviation was less than 2.5, which was determined by the aggregate of ten independent raters. \n![image](https://image.slidesharecdn.com/capstoneprojectgadatasciencelinm-160512021206/95/sentiment-analysis-of-airline-tweets-15-638.jpg)\n\nNow let's use VADER to analyze our movie reviews! We build our own modeling function as follows. In our modeling function, we do some basic pre-processing but keep the punctuations and emoticons intact. Besides this, we use VADER to get the sentiment polarity and also proportion of the review text with regard to positive, neutral and negative sentiment. We also predict the final sentiment based on a user-input threshold for the aggregated sentiment polarity."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b4d27dd2c09fc7769d2bd332843cb19bd7d46c5"
      },
      "cell_type": "code",
      "source": "def analyze_sentiment_vader_lexicon(review, threshold=0.1, verbose=False):\n    # pre-process text\n    review = strip_html_tags(review)\n    review = remove_accented_chars(review)\n    review = expand_contractions(review)\n    \n    # analyze the sentiment for review\n    analyzer = SentimentIntensityAnalyzer()\n    scores = analyzer.polarity_scores(review)\n    # get aggregate scores and final sentiment\n    agg_score = scores['compound']\n    final_sentiment = 'positive' if agg_score >= threshold\\\n                                   else 'negative'\n    if verbose:\n        # display detailed sentiment statistics\n        positive = str(round(scores['pos'], 2)*100)+'%'\n        final = round(agg_score, 2)\n        negative = str(round(scores['neg'], 2)*100)+'%'\n        neutral = str(round(scores['neu'], 2)*100)+'%'\n        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive, negative, neutral]],\n                                        columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n                                                                      ['Predicted Sentiment', 'Polarity Score',\n                                                                       'Positive', 'Negative', 'Neutral']], \n                                                              labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n        display(sentiment_frame)\n    \n    return final_sentiment",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5b4306d59009132ce92100842d721955b09a8f3c"
      },
      "cell_type": "markdown",
      "source": "Let's see how our model classify our samples and compare with their actual values. Typically, VADER recommends using positive sentiment for aggregated polarity >= 0.5, neutral between [-0.5, 0.5], and negative for polarity < -0.5. We use a threshold of >= 0.4 for positive and < 0.4 for negative in our corpus. The following is the analysis of our sample reviews."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2193860494255c60779241aa02e7550c19b110b1"
      },
      "cell_type": "code",
      "source": "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n    print('REVIEW:', review)\n    print('Actual Sentiment:', sentiment)\n    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=True)    ",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "REVIEW: For those not in the know, the Asterix books are a hugely successful series of comic books about a village of indomitable Gauls who resist Caesar's invasion thanks to a magic potion that renders them invulnerable supermen. There have been several animated features (only one of them, The Twelve Tasks of Asterix really capturing the wit and spirit of the books despite being an original screen story) before a perfectly cast Christian Clavier and Gerard Depardieu took the lead roles in two live action adaptations that proved colossally successful throughout Europe but made no impression whatsoever in the English-speaking world. <br /><br />The uncut French version is great fun, but sadly does not appear to be available in a version with English subtitles outside of the UK DVD. While there's still no sign of a US theatrical or DVD release, the Miramax version of Asterix et Obelix: Mission Cleopatre is also on that DVD (and has played on UK TV), and you'll never guess what - it's been completely re-edited (at least 21 minutes gone) and dubbed into English. Maybe Harve mistook it for a Hong Kong movie - after all, he never saw a foreign film he didn't think couldn't be improved by heavy re-editing and shelving for a few years.<br /><br />Whereas Asterix et Obelix Contre Cesar was lovingly dubbed into English from a particularly good translation script by Terry Jones but otherwise left unaltered, that sort of thing really isn't the Miramax way. The results ain't good. The film was the best attempt to get the books mixture of slapstick, anachronisms and highbrow classical humorous asides to the screen, but a lot of the classical references are gone (such as the great Raft of the Medusa sight gag or the Cyrano de Bergerac references from Depardieu), alongside anything that seems too French or might slow the picture down, with the result that the first 20 minutes are now a real slog. Several punchlines to sequences are missing, Depardieu's part has been trimmed (his part was already fairly small because of his serious health problems during the shoot: the US version has been partially digitally regraded to change the unhealthy pallor of his face in the original!), and as usual with dubbing, because literal translations into English don't fit properly, lines are either rushed so much they're not funny anymore or the dialogue has been changed completely (a couple of these changes are admittedly funny, like one character dreaming of a world in which he could move his lips in French and hear the words in English).<br /><br />Not a total disaster, but very disappointing considering how good the full-length version is. It would be nice to think that Miramax would do a Shaolin Soccer and release both versions, but since they've shelved both films for two years since paying $45m for them (another classic case of Harvey's notorious chronic buyer's remorse: gee, wonder why Disney were so p****d at their overspending) and still have no release plans, that may just be too much wishful thinking.<br /><br />It's a real pity that such an accessible and entertaining film will now only be available to non-French speakers in such a clumsily bowdlerised version. It seems the plucky Gauls may have been able to defeat Caesar's legions but are no match for the Miramax jackboot.\nActual Sentiment: positive\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "     SENTIMENT STATS:                                         \n  Predicted Sentiment Polarity Score Positive Negative Neutral\n0            positive           0.51    11.0%    11.0%   78.0%",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"5\" halign=\"left\">SENTIMENT STATS:</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Predicted Sentiment</th>\n      <th>Polarity Score</th>\n      <th>Positive</th>\n      <th>Negative</th>\n      <th>Neutral</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>positive</td>\n      <td>0.51</td>\n      <td>11.0%</td>\n      <td>11.0%</td>\n      <td>78.0%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "REVIEW: Horrendously acted and completely laughable haunted-house horror flick that has an out of place Anna Paquin playing a neurotic teenager fighting off the \"things-that-go-bump-in-the-dark\" that are plaguing her and her family shortly after moving to their new home in Spain(?!). Little more than a geographically re-planted rip-off of \"The Shining\" and most notably \"The Others\", the weak-plotted \"Darkness\" is basically your typical run-of-the mill B-horror feature with a few predictable lame scares that can be seen by audiences a mile off (so to speak)! In retrospect I suppose I shouldn't have set my personal expectations quite as high for this movie to actually be good considering the well-known fact that it was shelved for nearly three years before finally being released around Christmas of last year in American cinemas across the country to what was ultimately lukewarm ticket-sales and very harsh reviews from critics. When will filmmakers ever learn that there's more to making movies (be it horror or otherwise) than just the fey possibility of a little financial gain? (Turkey-Zero Stars)\nActual Sentiment: negative\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "     SENTIMENT STATS:                  ...                               \n  Predicted Sentiment Polarity Score   ...               Negative Neutral\n0            negative          -0.96   ...    14.000000000000002%   81.0%\n\n[1 rows x 5 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"5\" halign=\"left\">SENTIMENT STATS:</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Predicted Sentiment</th>\n      <th>Polarity Score</th>\n      <th>Positive</th>\n      <th>Negative</th>\n      <th>Neutral</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>-0.96</td>\n      <td>5.0%</td>\n      <td>14.000000000000002%</td>\n      <td>81.0%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "REVIEW: This is the only David Zucker movie that does not spoof anything the first of its kind. The funniest movie of 98 with Night at the Roxbury right behind But I did not think Theres something about mary was funny so that doesnt count except for the frank and beans thing he he. Dont listen to the critics especially Roger Ebert he does not know solid entertainment just look at his reviews.Anyway see it you wont be dissapionted\nActual Sentiment: positive\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "     SENTIMENT STATS:                  ...                              \n  Predicted Sentiment Polarity Score   ...              Negative Neutral\n0            positive           0.71   ...    7.000000000000001%   82.0%\n\n[1 rows x 5 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"5\" halign=\"left\">SENTIMENT STATS:</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Predicted Sentiment</th>\n      <th>Polarity Score</th>\n      <th>Positive</th>\n      <th>Negative</th>\n      <th>Neutral</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>positive</td>\n      <td>0.71</td>\n      <td>11.0%</td>\n      <td>7.000000000000001%</td>\n      <td>82.0%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c9b06948526c9073badb0b3c458c12ff68fb0251"
      },
      "cell_type": "markdown",
      "source": "Let's try out our model on the complete test movie review corpus now and evaluate the model performance."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5753eaa44492ed3fdce08f043e0f1a2b03ac88b7"
      },
      "cell_type": "code",
      "source": "predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.5, verbose=False) for review in test_reviews]\n\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n                                  target_names=['positive', 'negative'])",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Model Performance metrics:\n------------------------------\nAccuracy:  72.07% \nPrecision: 72.79% \nRecall:    72.07% \nF1 Score:  71.81% \n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n    positive       0.77      0.63      0.69      4959\n    negative       0.69      0.81      0.75      5041\n\n   micro avg       0.72      0.72      0.72     10000\n   macro avg       0.73      0.72      0.72     10000\nweighted avg       0.73      0.72      0.72     10000\n\n\nPrediction Confusion Matrix:\n------------------------------\n                 Predicted:         \n                   positive negative\nActual: positive       3106     1853\n        negative        940     4101\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "38917b22908f0d2bddc8c36663ab4547e3f33c09"
      },
      "cell_type": "markdown",
      "source": "We get an overall F1-Score and model accuracy of 72%, which is quite similar to the AFINN based model. The AFINN based model only wins for very little, both models have a similar performance."
    },
    {
      "metadata": {
        "_uuid": "9fb3216788e64eba1d87e4bd8f991bea01926b1e"
      },
      "cell_type": "markdown",
      "source": "## Classifying Sentiment with Supervised Learning\n\n__Introduction:__\n\nWe will be building an automated sentiment text classification system in subsequent sections. The major steps to achieve this are mentioned as follows.\n1. Prepare train and test datasets (optionally a validation dataset)\n2. Pre-process and normalize text documents\n3. Feature engineering\n4. Model training\n5. Model prediction and evaluation\n\n![image](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-1-4842-2388-8_4/MediaObjects/427287_1_En_4_Fig2_HTML.jpg)<center>Blueprint for building an automated text classification system (Source: Text Analytics with Python, Apress 2016)</center>\n\nIn our scenario, documents indicate the movie reviews and classes indicate the review sentiments that can either be positive or negative, making it a binary classification problem. \n\n### Feature Engineering\nOur feature engineering techniques will be based on the Bag of Words model and the TF-IDF model.\n\nThe ***bag-of-words model*** is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier. The core principle is to convert text documents into numeric vectors. The dimension or size of each vector is N where N indicates all possible distinct words across the corpus of documents. Each document once transformed is a numeric vector of size N where the values or weights in the vector indicate the frequency of each word in that specific document. Hence the name bag of words because this model represents unstructured text into a bag of words without taking into account word positions, syntax, or semantics.\n![image](https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2017/05/bagofwords.004.jpeg?resize=800%2C203)\n\nThere are some potential problems which might arise with the Bag of Words model when it is used on large corpora. Since the feature vectors are based on absolute term frequencies, there might be some terms which occur frequently across all documents and these will tend to overshadow other terms in the feature set. The ***[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) model*** tries to combat this issue by using a scaling or normalizing factor in its computation. TF-IDF stands for Term Frequency-Inverse Document Frequency, which uses a combination of two metrics in its computation, namely: term frequency (tf) and inverse document frequency (idf). This technique was developed for ranking results for queries in search engines and now it is an indispensable model in the world of information retrieval and text analytics.\n![image](https://skymind.ai/images/wiki/tfidf.png?resize=40%2C20)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6eb8daecf8f1dbb9d6504b9a9130579b1df4291"
      },
      "cell_type": "code",
      "source": "# build BOW features on train reviews\ncv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\ncv_train_features = cv.fit_transform(norm_train_reviews)\n\n# build TFIDF features on train reviews\ntv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2), sublinear_tf=True)\ntv_train_features = tv.fit_transform(norm_train_reviews)\n\n\n# transform test reviews into features\ncv_test_features = cv.transform(norm_test_reviews)\ntv_test_features = tv.transform(norm_test_reviews)\n\nprint('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\nprint('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": "BOW model:> Train features shape: (40000, 2340032)  Test features shape: (10000, 2340032)\nTFIDF model:> Train features shape: (40000, 2340032)  Test features shape: (10000, 2340032)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0810ca86f1cb75dadd2b608ae44c3ecad317c5b4"
      },
      "cell_type": "markdown",
      "source": "### Traditional Supervised Machine Learning Models\n\nWe can now use some traditional supervised Machine Learning algorithms which work very well on text classification. We recommend using logistic regression, support vector machines, and multinomial Naïve Bayes models\n\n#### Model Training\nThe logistic regression is intended for binary (two-class) classification problems, where it will predict the probability of an instance belonging to the default class, which can be snapped into a 0 or 1 classification. In this case, we try to predict the probability that a given movie review will belong to one of the discrete classes.\n\n**<center>P(X) = P(Y=1|X)</center>**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ecb7bc06e67f5ec5fa7af45ce14c98d7525516cc"
      },
      "cell_type": "code",
      "source": "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\nsvm = SGDClassifier(loss='hinge', l1_ratio=0.15, max_iter=300, n_jobs=4, random_state=101)",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f9d3ac8a432a8dae99976428e22bdf0d14b27031"
      },
      "cell_type": "markdown",
      "source": "#### Prediction and Performance Evaluation\nWe will now use our utility function train_predict_model(...) to build a logistic regression model on our training features and evaluate the model performance on our test features."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90deceaf556064d66aff23e0b19eaf4631ad86d8"
      },
      "cell_type": "code",
      "source": "# Logistic Regression model on BOW features\nlr_bow_predictions = train_predict_model(classifier=lr, \n                                         train_features=cv_train_features, train_labels=train_sentiments,\n                                         test_features=cv_test_features, test_labels=test_sentiments)\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\n                                  target_names=['positive', 'negative'])",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Model Performance metrics:\n------------------------------\nAccuracy:  90.79% \nPrecision: 90.79% \nRecall:    90.79% \nF1 Score:  90.79% \n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n    positive       0.91      0.91      0.91      4959\n    negative       0.91      0.91      0.91      5041\n\n   micro avg       0.91      0.91      0.91     10000\n   macro avg       0.91      0.91      0.91     10000\nweighted avg       0.91      0.91      0.91     10000\n\n\nPrediction Confusion Matrix:\n------------------------------\n                 Predicted:         \n                   positive negative\nActual: positive       4493      466\n        negative        455     4586\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "2debfa653fd399dec8a68101938cbac66f48c0e1"
      },
      "cell_type": "markdown",
      "source": "We get all metrics as **91%**, which is really excellent! \n\nWe can now build a logistic regression model similarly on our TF-IDF features and see if we can get better results."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e5a5bb8ce926a6fd7733ce1cbaa290ef5e48c6f"
      },
      "cell_type": "code",
      "source": "# Logistic Regression model on TF-IDF features\nlr_tfidf_predictions = train_predict_model(classifier=lr, \n                                           train_features=tv_train_features, train_labels=train_sentiments,\n                                           test_features=tv_test_features, test_labels=test_sentiments)\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\n                                  target_names=['positive', 'negative'])",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Model Performance metrics:\n------------------------------\nAccuracy:  89.94% \nPrecision: 89.94% \nRecall:    89.94% \nF1 Score:  89.94% \n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n    positive       0.90      0.89      0.90      4959\n    negative       0.90      0.91      0.90      5041\n\n   micro avg       0.90      0.90      0.90     10000\n   macro avg       0.90      0.90      0.90     10000\nweighted avg       0.90      0.90      0.90     10000\n\n\nPrediction Confusion Matrix:\n------------------------------\n                 Predicted:         \n                   positive negative\nActual: positive       4427      532\n        negative        474     4567\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5ef351db31a16bfe0590dfb871ddfb99d2ad1cdd"
      },
      "cell_type": "markdown",
      "source": "As you can see we get all metrics close to 90%, which which is great but our previous model is still slightly better.\n\nLet's see if we can do better with SVM:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84646992bb226f061ac3e5eca4ab8147b5447388"
      },
      "cell_type": "code",
      "source": "svm_bow_predictions = train_predict_model(classifier=svm, \n                                          train_features=cv_train_features, train_labels=train_sentiments,\n                                          test_features=cv_test_features, test_labels=test_sentiments)\nprint('SVM results with Bow:')\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_bow_predictions,\n                                 target_names=['positive', 'negative'])\n\n\nsvm_tfidf_predictions = train_predict_model(classifier=svm, \n                                            train_features=tv_train_features, train_labels=train_sentiments,\n                                            test_features=tv_test_features, test_labels=test_sentiments)\nprint('-'*60)\nprint('\\nSVM results with TF-IDF:')\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_tfidf_predictions,\n                                  target_names=['positive', 'negative'])",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "SVM results with Bow:\nModel Performance metrics:\n------------------------------\nAccuracy:  90.86% \nPrecision: 90.87% \nRecall:    90.86% \nF1 Score:  90.86% \n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n    positive       0.91      0.90      0.91      4959\n    negative       0.90      0.92      0.91      5041\n\n   micro avg       0.91      0.91      0.91     10000\n   macro avg       0.91      0.91      0.91     10000\nweighted avg       0.91      0.91      0.91     10000\n\n\nPrediction Confusion Matrix:\n------------------------------\n                 Predicted:         \n                   positive negative\nActual: positive       4471      488\n        negative        426     4615\n--------------------------------------------------------------------------------\n\nSVM results with TF-IDF:\nModel Performance metrics:\n------------------------------\nAccuracy:  90.24% \nPrecision: 90.26% \nRecall:    90.24% \nF1 Score:  90.24% \n\nModel Classification report:\n------------------------------\n              precision    recall  f1-score   support\n\n    positive       0.91      0.89      0.90      4959\n    negative       0.89      0.92      0.90      5041\n\n   micro avg       0.90      0.90      0.90     10000\n   macro avg       0.90      0.90      0.90     10000\nweighted avg       0.90      0.90      0.90     10000\n\n\nPrediction Confusion Matrix:\n------------------------------\n                 Predicted:         \n                   positive negative\nActual: positive       4407      552\n        negative        424     4617\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c2ab240299853685ff263444078780dcf2e23e01"
      },
      "cell_type": "markdown",
      "source": "As you can see, again we obtened all scores close to 90%. Let's see if we can improve with apply a DNN model.\n\n### Newer Supervised Deep Learning Models\nIn this section, we will be building some deep neural networks and train them on some advanced text features based on word embeddings to build a text sentiment classification system.\n\n[![image](http://159.89.224.205/wp-content/uploads/2016/07/tumblr_inline_oabas5sThb1sleek4_540.png)](http://blog.aylien.com/leveraging-deep-learning-for-multilingual/)"
    },
    {
      "metadata": {
        "_uuid": "12b1db6a6d5eec5fbce223b71b1cb8a59e55208c"
      },
      "cell_type": "markdown",
      "source": "#### Prediction class label encoding\n\nThe following snippet helps us tokenize our movie reviews and also converts the text-based sentiment class labels into one-hot encoded vectors."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5930f641af9e12fc759feb8a05aaba7fba15153"
      },
      "cell_type": "code",
      "source": "le = LabelEncoder()\nnum_classes=2  # positive -> 1, negative -> 0\n\n# tokenize train reviews & encode train labels\ntokenized_train = [tokenizer.tokenize(text) for text in norm_train_reviews]\ny_tr = le.fit_transform(train_sentiments)\ny_train = keras.utils.to_categorical(y_tr, num_classes)\n\n# tokenize test reviews & encode test labels\ntokenized_test = [tokenizer.tokenize(text) for text in norm_test_reviews]\ny_ts = le.fit_transform(test_sentiments)\ny_test = keras.utils.to_categorical(y_ts, num_classes)\n\n# print class label encoding map and encoded labels\nprint('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\nprint('Sample test label transformation:\\n'+'-'*35,\n      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_ts[:3], \n      '\\nOne hot encoded Labels:\\n', y_test[:3])",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Sentiment class label map: {'negative': 0, 'positive': 1}\nSample test label transformation:\n----------------------------------- \nActual Labels: ['positive' 'positive' 'positive'] \nEncoded Labels: [1 1 1] \nOne hot encoded Labels:\n [[0. 1.]\n [0. 1.]\n [0. 1.]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1b8f67d733ee1af1738e51300b087fd14650d5d1"
      },
      "cell_type": "markdown",
      "source": "Thus, we can see from the preceding sample outputs how our sentiment class labels have been encoded into numeric representations, which in turn have been converted into one-hot encoded vectors. \n\n#### Feature Engineering with word embeddings\nBasically, ***word embeddings*** can be used for **feature extraction** and **language modeling**. This representation tries to map each word or phrase into a complete numeric vector such that semantically similar words or terms tend to occur closer to each other and these can be quantified using these embeddings. \n\nThe ***word2vec model*** was built by Google is perhaps one of the most popular neural network based probabilistic language models and can be used to learn distributed representational vectors for words. Word embeddings produced by word2vec involve taking in a corpus of text documents, representing words in a large high dimensional vector space such that each word has a corresponding vector in that space and similar words (even semantically) are located close to one another.\n\nWe will be using the gensim framework to implement the same model of word2vec created by Google, on our corpus to extract features. Some of the important parameters in the model are explained briefly as follows:\n- **size**: Represents the feature vector size for each word in the corpus when transformed.\n- **window**: Sets the context window size specifying the length of the window of words to be taken into account as belonging to a single, similar context when training.\n- **min_count**: Specifies the minimum word frequency value needed across the corpus to consider the word as a part of the final vocabulary during training the model.\n- **sample**: Used to downsample the effects of words which occur very frequently."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b83363b184748ea914a70d17070744b05204cdb"
      },
      "cell_type": "code",
      "source": "# build word2vec model\nw2v_num_features = 500\nw2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150, min_count=10, sample=1e-3)    ",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ca8e7c83e7c17141f57476e0d7d9fb7b4af3881a"
      },
      "cell_type": "markdown",
      "source": "Each word in the corpus with at least 10 counts will essentially now be a vector itself of size 500. \n\nA question might arise in your mind now that so far, we had feature vectors for each complete document, but now we have vectors for each word. How do we represent entire documents now? We can do that using various aggregation and combinations. A simple scheme would be to use an averaged word vector representation, where we simply sum all the word vectors occurring in a document and then divide by the count of word vectors to represent an averaged word vector for the document. The following code enables us to do the same."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f530c091c39f921cc3d72283b1eba75c7c11b204"
      },
      "cell_type": "code",
      "source": "def averaged_word2vec_vectorizer(corpus, model, num_features):\n    vocabulary = set(model.wv.index2word)\n    \n    def average_word_vectors(words, model, vocabulary, num_features):\n        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n        nwords = 0.\n        \n        for word in words:\n            if word in vocabulary: \n                nwords = nwords + 1.\n                feature_vector = np.add(feature_vector, model[word])\n        if nwords:\n            feature_vector = np.divide(feature_vector, nwords)\n\n        return feature_vector\n\n    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n                    for tokenized_sentence in corpus]\n    return np.array(features)",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "08503648d5ecbf74c96404772657680cd7c55636"
      },
      "cell_type": "markdown",
      "source": "We can now use the previous function to generate averaged word vector representations on our two movie review datasets."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71db8e026e88ec8af4c195b9cd88cea0d19687b5"
      },
      "cell_type": "code",
      "source": "# generate averaged word vector features from word2vec model\navg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model, num_features=w2v_num_features)\navg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model, num_features=w2v_num_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2cfc390bb29da0aa06b33dcfae850b4e48c023c0"
      },
      "cell_type": "markdown",
      "source": "We complete our generate embeddings by using the Global Vectors for Word Representation (GloVe) models, is an unsupervised model for obtaining word vector representations. Created at [Stanford University](https://nlp.stanford.edu/pubs/glove.pdf#_blank), this model is trained on various corpora like Wikipedia, Common Crawl, and Twitter and corresponding pre-trained word vectors are available that can be used for our analysis needs. \n\nThe spacy library provided 384-dimensional word vectors trained on the Common Crawl corpus using the GloVe model. They provide a simple standard interface to get feature vectors of size 384 for each word as well as the averaged feature vector of a complete text document. \n\nCheck on the [GloVe project site](https://nlp.stanford.edu/projects/glove) others pre-trained models and examples."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a0b6afb35aaff583687be73d7ca76865de9baa59"
      },
      "cell_type": "code",
      "source": "# feature engineering with GloVe model\ntrain_nlp = [nlp(item) for item in norm_train_reviews]\ntrain_glove_features = np.array([item.vector for item in train_nlp])\n\ntest_nlp = [nlp(item) for item in norm_test_reviews]\ntest_glove_features = np.array([item.vector for item in test_nlp])\n\nprint('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)\nprint('GloVe model:> Train features shape:', train_glove_features.shape, ' Test features shape:', test_glove_features.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7913680a13e585608f6a038e1de0b9c21c872762"
      },
      "cell_type": "markdown",
      "source": "#### Modeling with deep neural networks \n[![image](https://www.mdpi.com/algorithms/algorithms-09-00041/article_deploy/html/images/algorithms-09-00041-g002.png)](https://www.mdpi.com/1999-4893/9/2/41/htm)\n##### Building Deep neural network architecture\nWe will be using a fully-connected four layer deep neural network (multi-layer perceptron or deep ANN) for our model. We call this a fully connected deep neural network (DNN) because neurons or units in each pair of adjacent layers are fully pairwise connected. These networks are also known as deep artificial neural networks (ANNs) or Multi-Layer Perceptrons (MLPs) since they have more than one hidden layer. The following function leverages keras on top of tensorflow to build the desired DNN model. We build a Sequential model, which helps us linearly stack our hidden and output layers."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "175e6b5a92f794e8b3527607f96db8fc83c6ed4b"
      },
      "cell_type": "code",
      "source": "def construct_deepnn_architecture(num_input_features):\n    dnn_model = Sequential()\n    dnn_model.add(Dense(1024, activation='relu', input_shape=(num_input_features,)))\n    dnn_model.add(Dropout(0.5))\n    dnn_model.add(Dense(1024, activation='relu'))\n    dnn_model.add(Dropout(0.5))\n    dnn_model.add(Dense(512, activation='relu'))\n    dnn_model.add(Dropout(0.2))\n    dnn_model.add(Dense(2, activation='softmax'))\n\n    dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return dnn_model\n\nw2v_dnn = construct_deepnn_architecture(num_input_features=500)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5caa1f7be02db409eb17068218ccf83d53b0ee72"
      },
      "cell_type": "markdown",
      "source": "We do not count the input layer usually in any deep architecture, hence our model will consist of three hidden layers of 512 neurons or units and one output layer with two units that will be used to either predict a positive or negative sentiment based on the input layer features.\n\nWe use 1024, 1024 and 512 units for our hidden layers respectively and the **activation function relu** indicates a ***rectified linear unit***.  This function tries to solve the ***vanishing gradient problem***. This problem occurs when x > 0 and as x increases, where x is typically the input to a neuron, the gradient from sigmoids becomes really small (almost vanishing) but relu prevents this from happening. Besides this, it also ***helps with faster convergence of gradient descent***. \n\nWe also use regularization in the network in the form of ***Dropout layers***. By adding a **dropout rates of 0.5 and 0.2**, it randomly sets 50% and 20% of the input feature units to 0 at each update during training the model. This form of regularization ***helps prevent overfitting the model***.\n\nThe final output layer consists of two units with a ***softmax activation function***. The softmax function is basically a generalization of the **logistic function**, which can be used to represent a probability distribution over n possible class outcomes. In our case n = 2 where the class can either be positive or negative and the softmax probabilities will help us determine the same. \n\nThe compile(...) method is used to configure the learning or training process of the DNN model before we actually train it. This involves providing a [***cost*** or ***loss function***](https://keras.io/losses/) in the loss parameter. This will be the goal or objective which the model will try to minimize. \n\nWe will be using ***binary_crossentropy***, which helps us minimize the error or loss from the softmax output. We need an optimizer for helping us converge our model and minimize the loss or error function. Gradient descent or stochastic gradient descent is a popular optimizer. We will be using the ***rmsprop optimizer***, other option is [adam](https://arxiv.org/pdf/1412.6980v8.pdf ) also uses momentum where basically each update is based on not only the gradient computation of the current point but also includes a fraction of the previous update. This helps with faster convergence. \n\nLet's visualize our deep architecture:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41e17f28809badd54b71ca2db8d02c8b0b5c0cb0"
      },
      "cell_type": "code",
      "source": "SVG(model_to_dot(w2v_dnn, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4ca740adf1e9974c2523a86fd1a01bed433b3a4d"
      },
      "cell_type": "markdown",
      "source": "##### Model Training, Prediction and Performance Evaluation\nWe will be using the fit(...) function from keras for the training process and there are some parameters which you should be aware of:\n- **epoch**: indicates one complete forward and backward pass of all the training examples through the network. \n- **batch_size**: indicates the total number of samples which are propagated through the DNN model at a time for one backward and forward pass for training the model and updating the gradient. \n- **validation_split**: we use 0.15 to extract 15% of the training data and use it as a validation dataset for evaluating the performance at each epoch. \n- **shuffle**: helps shuffle the samples in each epoch when training the model. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0693696692863a01613b687f3705e26a8870536a"
      },
      "cell_type": "code",
      "source": "batch_size = 64\nw2v_dnn.fit(avg_wv_train_features, y_train, epochs=15, batch_size=batch_size, shuffle=True, \n            validation_split=0.15, verbose=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "56fe88751361f2ba2531a93cf247411d6b00d0d9"
      },
      "cell_type": "markdown",
      "source": "Let's evaluate our model performance on the test review word2vec features:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc76da81efa4f82bcf92c6620412ef0c75f6d944"
      },
      "cell_type": "code",
      "source": "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\npredictions = le.inverse_transform(y_pred) \nprint('-'*60)\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n                                  target_names=['positive', 'negative'])  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ebe7de4520598ce197d09b0ce0171ba258695f56"
      },
      "cell_type": "markdown",
      "source": "The results is close to 88% in all metrics. As you see, the results were improve with the increase of epochs, but you need take care with the overfitting, maybe need try others DNN configurations to you can get better results! \n\nLet's see how our DNN model perform with our GloVe based features: "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67058627d16cc24cf7b9c460832dd10029f1a766"
      },
      "cell_type": "code",
      "source": "glove_dnn = construct_deepnn_architecture(num_input_features=384)\n\nbatch_size = 64\nglove_dnn.fit(train_glove_features, y_train, epochs=15, batch_size=batch_size, shuffle=True, \n              validation_split=0.15, verbose=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7ae147800fe67f4525b20946d693e061063d5496"
      },
      "cell_type": "markdown",
      "source": "As expected, this model was somewhat lower given the reduction of inputs, however it seems more resistant to overfitting, it allow us to observe the potential in using pre-trained models. \n\nLet's take a look at the behavior of this model versus the test base:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ebc72add632078151765e9ee5cd488c027128d6"
      },
      "cell_type": "code",
      "source": "y_pred = glove_dnn.predict_classes(test_glove_features)\npredictions = le.inverse_transform(y_pred) \nprint('-'*80)\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n                                  target_names=['positive', 'negative'])  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ec4dd281a8105ee8d8c1d19b331e5f427f1b250f"
      },
      "cell_type": "markdown",
      "source": "## Advanced Supervised Deep Learning Models\n\nIn this section we will use a more advanced models than your regular fully connected deep networks, a recurrent neural networks (RNNs) and long short term memory networks (LSTMs) which also considers the sequence of data (words,events, and so on). More over the Bidirectional lstms keep the contextual information in both directions.\n\n![image](http://thelillysblog.com/images/architecture-nn2.jpg)\n\n### Preparing data\n\nWe will start with the procedures to preparing the data for our needs on the RNN and LSTM.\n\n#### Tokenize train & test datasets\n\nThe following snippet helps us tokenize our movie reviews."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c3005743b03a22179bccaa2db5756062620a315"
      },
      "cell_type": "code",
      "source": "tokenized_train = [tokenizer.tokenize(text) for text in norm_train_reviews]\ntokenized_test = [tokenizer.tokenize(text) for text in norm_test_reviews]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9013224d5109f8695461c19f83335c8eca857851"
      },
      "cell_type": "markdown",
      "source": "#### Build Vocabulary Mapping (word to index)\nFor feature engineering, we will be creating word embeddings. Word embeddings tend to vectorize text documents into fixed sized vectors such that these vectors try to capture contextual and semantic information.\n\nFor generating embeddings, we will use the Embedding layer from keras, which requires documents to be represented as tokenized and numeric vectors. We already have tokenized text vectors, so we would need to convert them into numeric representations. Besides this, we would also need the vectors to be of uniform size even though the tokenized text reviews will be of variable length due to the difference in number of tokens in each review. For this, one strategy could be to take the length of the longest review (with maximum number of tokens\\words) and set it as the vector size, let's call this max_len. Reviews of shorter length can be padded with a PAD term in the beginning to increase their length to max_len.\n\nWe would need to create a word to index vocabulary mapping for representing each tokenized text review in a numeric form. Do note you would also need to create a numeric mapping for the padding term which we shall call PAD_INDEX and assign it the numeric index of 0. For unknown terms, in case they are encountered later on in the test dataset or newer, previously unseen reviews, we would need to assign it to some index too. This would be because we will vectorize, engineer features, and build models only on the training data. Hence, if some new term should come up in the future, we will consider it as an out of vocabulary (OOV) term and assign it to a constant index."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c76d030e29525453bef9a1fa318fca345aa253a"
      },
      "cell_type": "code",
      "source": "# build word to index vocabulary\ntoken_counter = Counter([token for review in tokenized_train for token in review])\nvocab_map = {item[0]: index+1 for index, item in enumerate(dict(token_counter).items())}\nmax_index = np.max(list(vocab_map.values()))\nvocab_map['PAD_INDEX'] = 0\nvocab_map['NOT_FOUND_INDEX'] = max_index+1\nvocab_size = len(vocab_map)\n\n# view vocabulary size and part of the vocabulary map\nprint('Vocabulary Size:', vocab_size)\nprint('Sample slice of vocabulary map:\\n', dict(list(vocab_map.items())[10:20]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "593bdba3371fa27c59fc00fc5e1b67332ab1c128"
      },
      "cell_type": "markdown",
      "source": "You may notice that we have used all the terms found in training dataset in our vocabulary. As alternative, you can easily filter and use more relevant terms here, based on their frequency, by using the most_common(count) function from Counter and taking the first count terms from the list of unique terms in the training corpus.\n\n#### Encode and Pad datasets & Encode prediction class labels\n\nThe following snippet helps us encode and pad our movie reviews encode the tokenized text reviews based on the previous vocab_map. Also converts the text-based sentiment class labels into one-hot encoded vectors."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6df84205a1b4c59089c7fe0cd60e69d1cac59b91"
      },
      "cell_type": "code",
      "source": "# get max length of train corpus and initialize label encoder\nle = LabelEncoder()\nnum_classes=2 # positive -> 1, negative -> 0\nmax_len = np.max([len(review) for review in tokenized_train])\n\n## Train reviews data corpus\n# Convert tokenized text reviews to numeric vectors\ntrain_X = [[vocab_map[token] for token in tokenized_review] for tokenized_review in tokenized_train]\ntrain_X = sequence.pad_sequences(train_X, maxlen=max_len) # pad \n## Train prediction class labels\n# Convert text sentiment labels (negative\\positive) to binary encodings (0/1)\ntrain_y = le.fit_transform(train_sentiments)\n\n## Test reviews data corpus\n# Convert tokenized text reviews to numeric vectors\ntest_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map['NOT_FOUND_INDEX'] \n           for token in tokenized_review] \n              for tokenized_review in tokenized_test]\ntest_X = sequence.pad_sequences(test_X, maxlen=max_len)\n## Test prediction class labels\n# Convert text sentiment labels (negative\\positive) to binary encodings (0/1)\ntest_y = le.transform(test_sentiments)\n\n# view vector shapes\nprint('Max length of train review vectors:', max_len)\nprint('Train review vectors shape:', train_X.shape, ' Test review vectors shape:', test_X.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "306d44313b31b1ca3c0fa5c1efe871e2a1ef78fe"
      },
      "cell_type": "markdown",
      "source": "### Build the LSTM Model Architecture\nLet's introducing the ***Embedding layer*** and coupling it with the deep network architecture based on ***LSTMs***.\n\nThe **Embedding layer** helps us generate the word embeddings from scratch. This layer is also initialized with some weights initially and this gets updated based on our optimizer similar to weights on the neuron units in other layers when the network tries to minimize the loss in each epoch. Thus, the embedding layer tries to optimize its weights such that we get the best word embeddings which will generate minimum error in the model and also capture semantic similarity and relationships among words.\n\n**LSTMs** basically try to overcome the shortcomings of RNN models especially with regard to handling long term dependencies and problems which occur when the weight matrix associated with the units/neurons become too small,***leading to vanishing gradient***, or too large, ***leading to exploding gradient***. The RNN units usually have a chain of repeating modules such that the module has a simple structure of having maybe one layer with the tanh activation. LSTMs are also a special type of RNN, having a similar structure but the LSTM unit has four neural network layers instead of just one. A **Bidirectional LSTM Layer** connects two hidden layers of opposite directions to the same output.\n![image](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n\nIn the diagram below, the notation t indicates one time step, C depicts the cell states, and h indicates the hidden states. The gates i, f, o and c̅ help in removing or adding information to the cell state. The gates i, f and o represent the input, output and forget gates respectively and each of them are modulated by the sigmoid layer which outputs numbers from 0 to 1 controlling how much of the output from these gates should pass. Thus this helps is protecting and controlling the cell state.\n![image](https://i.stack.imgur.com/aTDpS.png)\nFor a detailed work flow of how information flows through the LSTM cell consult the [Christopher Olah’s blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n\nThe final layer in our deep network is the Dense layer with 1 unit and the sigmoid activation function. We basically use the binary_crossentropy function with the adam optimizer since this is a binary classification problem and the model will ultimately predict a 0 or a 1, which we can decode back to a negative or positive sentiment prediction with our label encoder. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f60dca968a37ef75d59ca78ed09b0630dc3183aa"
      },
      "cell_type": "code",
      "source": "EMBEDDING_DIM = 128 # dimension for dense embeddings for each token\nLSTM_DIM = 64 # total LSTM units\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len))\nmodel.add(SpatialDropout1D(0.3))\n#model.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.3))\nmodel.add( Bidirectional( LSTM(lstm_out = 196, dropout_U = 0.2, dropout_W = 0.2)))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6573fd42ff542fbfac35e2bcbd450ef9a1b15536"
      },
      "cell_type": "markdown",
      "source": "#### Visualize model architecture"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d35810012dc3bed777d4e3b6b1ac23ed1f07a55"
      },
      "cell_type": "code",
      "source": "print(model.summary())\nSVG(model_to_dot(model, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "015980c60f92d0e51e5b8309d49756ec8e094d0e"
      },
      "cell_type": "markdown",
      "source": "### Train the model\nTraining LSTMs on CPU is notoriously slow. Of course, a  GPU based Deep Learning environment or a cloud-based environment, like Google Cloud Platform or AWS on GPU, took approximately at least less than four times to train the same model. So I would recommend you choose GPU environment, especially when working with RNNs or LSTM based network architectures."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c475d2b8aee7fbca03cec29b5b74314dae6966f"
      },
      "cell_type": "code",
      "source": "batch_size = 100\nmodel.fit(train_X, train_y, epochs=5, batch_size=batch_size, shuffle=True, validation_split=0.1, verbose=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8ddd8425428389d174a2bc68631c064f6dfafe49"
      },
      "cell_type": "markdown",
      "source": "Based on the preceding output, we can see that just with five epochs we have decent validation accuracy, but like before, validation accuracy was nor better and the training accuracy starts shooting up indicating some over-fitting might be happening. Ways to overcome this include adding more data or by increasing the drouput rate. \n\n### Predict and Evaluate Model Performance"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "760427825cebe0a4fa37cfd2007fa0877811b210"
      },
      "cell_type": "code",
      "source": "pred_test = model.predict_classes(test_X)\npredictions = le.inverse_transform(pred_test.flatten())\n\ndisplay_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n                                  target_names=['positive', 'negative'])  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d10a366e5100af3998b27a2b2c1931f39b4ea587"
      },
      "cell_type": "markdown",
      "source": "Like in the other deep learning architecture we get close to 88% at all metrics, which is quite good! With more quality data, you can expect to get even better results. Try experimenting with different architectures and see if you get better results!\n\n## Analyzing Sentiment Causation\n\nBusiness and key stakeholders often perceive Machine Learning models as complex black boxes and poses the question, why should I trust your model? Explaining to them complex mathematical or theoretical concepts doesn't serve the purpose. Is there some way in which we can explain these models in an easy-to-interpret manner?\n\n[![image](https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/video_screenshot.png)](https://www.youtube.com/watch?v=hUnRCxnydCc)\n\nIn the analyze sentiment causation, the main idea is to determine the root cause or key factors causing positive or negative sentiment. The first area of focus will be model interpretation, where we will try to understand, interpret, and explain the mechanics behind predictions made by our classification models. The second area of focus is to apply topic modeling and extract key topics from positive and negative sentiment reviews.\n\n### Build Text Classification Pipeline with The Best Model\nLet's first build a basic text classification pipeline for the model that worked best for us so far. This is the Logistic Regression model based on the Bag of Words feature model. We will leverage the pipeline module from scikit-learn to build this Machine Learning pipeline using the following code."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f62d90ce0a35cab2ee621fa53c3a397de463daa6"
      },
      "cell_type": "code",
      "source": "# build BOW features on train reviews\ncv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\ncv_train_features = cv.fit_transform(norm_train_reviews)\n\n# build Logistic Regression model\nlr = LogisticRegression()\nlr.fit(cv_train_features, train_sentiments)\n\n# Build Text Classification Pipeline\nlr_pipeline = make_pipeline(cv, lr)\n\n# save the list of prediction classes (positive, negative)\nclasses = list(lr_pipeline.classes_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "51de91487af5ab0c42445e5e99bb0510e865856d"
      },
      "cell_type": "markdown",
      "source": "We build our model based on norm_train_reviews, which contains the normalized training reviews that we have used in all our earlier analyses. Now that we have our classification pipeline ready, you can actually deploy the model by using pickle or joblib to save the classifier and feature objects \n\n### Interpreting Predictive Models\nThere are various ways to interpret the predictions made by our predictive sentiment classification models. We want to understand more into why a positive review was correctly predicted as having positive sentiment or a negative review having negative sentiment. Besides this, no model is a 100% accurate always, so we would also want to understand the reason for mis-classifications or wrong predictions. \n\n#### Analyze Model Prediction Probabilities\nAssuming our pipeline is in production, how do we use it for new movie reviews? Let's try to predict the sentiment for two new sample reviews, which were not used in training the model:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c05a4ee61b25db5bdbaf70fe212c603a005f13f"
      },
      "cell_type": "code",
      "source": "lr_pipeline.predict(['the lord of the rings is an excellent movie', 'i hated the recent movie on tv, it was so bad'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bcf5449cc17bdac5456968813efbba07fba91595"
      },
      "cell_type": "markdown",
      "source": "Our classification pipeline predicts the sentiment of both the reviews correctly! This is a good start, but how do we interpret the model predictions? One way is to typically use the model prediction class probabilities as a measure of confidence. You can use the following code to get the prediction probabilities for our sample reviews."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6dc20a22012fee8064a9f332da54559c634a6bd0"
      },
      "cell_type": "code",
      "source": "pd.DataFrame(lr_pipeline.predict_proba(['the lord of the rings is an excellent movie', \n                     'i hated the recent movie on tv, it was so bad']), columns=classes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4480548252cd12b4d67d54ac5ed40dd2d8e883e9"
      },
      "cell_type": "markdown",
      "source": "Thus we can say that the first movie review has a prediction confidence or probability of 83% to have positive sentiment as compared to the second movie review with a 73% probability to have negative sentiment. \n\n#### Interpreting Model Decisions\nBesides prediction probabilities, we will be using the [skater framework](https://github.com/marcotcr/lime) for easy interpretation of the model decisions. First, to do this we define a helper function which takes in a document index, a corpus, its response predictions, and an explainer object and helps us with the our model interpretation analysis."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "392cc0dfd514451a772d9e304d422221d6039768"
      },
      "cell_type": "code",
      "source": "explainer = LimeTextExplainer(class_names=classes)\ndef interpret_classification_model_prediction(doc_index, norm_corpus, corpus, prediction_labels, explainer_obj):\n    # display model prediction and actual sentiments\n    print(\"Test document index: {index}\\nActual sentiment: {actual}\\nPredicted sentiment: {predicted}\"\n      .format(index=doc_index, actual=prediction_labels[doc_index],\n              predicted=lr_pipeline.predict([norm_corpus[doc_index]])))\n    # display actual review content\n    print(\"\\nReview:\", corpus[doc_index])\n    # display prediction probabilities\n    print(\"\\nModel Prediction Probabilities:\")\n    for probs in zip(classes, lr_pipeline.predict_proba([norm_corpus[doc_index]])[0]):\n        print(probs)\n    # display model prediction interpretation\n    exp = explainer.explain_instance(norm_corpus[doc_index], \n                                     lr_pipeline.predict_proba, num_features=10, \n                                     labels=[1])\n    exp.show_in_notebook()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "76f3f0211b4736b4530d8307d00edefb38f22065"
      },
      "cell_type": "markdown",
      "source": "The preceding snippet leverages skater to explain our text classifier to analyze its decision-making process in a global perspective. This is done by learning the model around the vicinity of the data point of interest X by sampling instances around X and assigning weightages based on their proximity to X. Thus, these locally learned linear models help in explaining complex models in a more easy to interpret way with class probabilities, contribution of top features to the class probabilities that aid in the decision making process. \n[![image](https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/lime.png)](https://arxiv.org/pdf/1602.04938.pdf)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e21799894ecaf47c105cbc4f893da5048bd07ce"
      },
      "cell_type": "code",
      "source": "doc_index = 100 \ninterpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n                                         corpus=test_reviews, prediction_labels=test_sentiments,\n                                         explainer_obj=explainer)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b879a4d302f4b8a9996a1a2d3e2269a894a10ff8"
      },
      "cell_type": "markdown",
      "source": "The results show us the top 10 features and we can notice that our model performs quite well in this scenario. Besides this, the word great contributed the maximum to the positive probability of 0.16 and in fact if we had removed this word from our review text, the positive probability would have dropped significantly.\n\nLet's see a positive classification case:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c3f7827ae310c2002179ea600ce00a397d911d44"
      },
      "cell_type": "code",
      "source": "doc_index = 2000\ninterpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n                                         corpus=test_reviews, prediction_labels=test_sentiments,\n                                         explainer_obj=explainer)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8ec1eb17f86cdcb4b0893a93d520c2a58abf4fe9"
      },
      "cell_type": "markdown",
      "source": "Based on the content, the reviewer really liked this model and also it was a real cult classic among certain age groups. In our final analysis, we will look at the model interpretation of an example where the model makes a wrong prediction."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e7b430ec1de71869b36a400cb13616b44e2d0c8d"
      },
      "cell_type": "code",
      "source": "doc_index = 347 \ninterpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n                                         corpus=test_reviews, prediction_labels=test_sentiments,\n                                         explainer_obj=explainer)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fcb519212871f95cccf0bc422f9d2f3fc240e00c"
      },
      "cell_type": "markdown",
      "source": "The results tell us that the reviewer in fact shows signs of positive sentiment in the movie review, especially in parts where he\\she tells us that “I loved it. I still think the directing and cinematography are excellent, as is the music... Alan Rickman is great, a bit old perhaps, but he plays the role beautifully. And Elizabeth Spriggs, she is absolutely fantastic as always.” and feature words from the same have been depicted in the top features contributing to positive sentiment. The model interpretation also correctly identifies the aspects of the review contributing to negative sentiment like, “But it's really the script that has over the time started to bother me more and more.”. Hence, this is one of the more complex reviews which indicate both positive and negative sentiment and the final interpretation would be in the reader's hands. \n\n### Analyzing Topic Models\nThe main aim of topic models is to extract and depict key topics or concepts which are otherwise latent and not very prominent in huge corpora of text documents. \n\nFor do this we can use some topic modeling technique like Latent Dirichlet Allocation (LDA) or Non-Negative Matrix\nfactorization. let's proceed with the second one.\n\n#### Extract features from positive and negative reviews\nThe first step in this analysis is to combine all our normalized train and test reviews and separate out these reviews into positive and negative sentiment reviews. Once we do this, we will extract features from these two datasets using the TF-IDF feature vectorizer. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c21950bbb48d362219c0a3d3779405c74fd34ae"
      },
      "cell_type": "code",
      "source": "# consolidate all normalized reviews\nnorm_reviews = norm_train_reviews+norm_test_reviews\n\n# get tf-idf features for only positive reviews\npositive_reviews = [review for review, sentiment in zip(norm_reviews, sentiments) if sentiment == 'positive']\nptvf = TfidfVectorizer(use_idf=True, min_df=0.05, max_df=0.95, ngram_range=(1,1), sublinear_tf=True)\nptvf_features = ptvf.fit_transform(positive_reviews)\n\n# get tf-idf features for only negative reviews\nnegative_reviews = [review for review, sentiment in zip(norm_reviews, sentiments) if sentiment == 'negative']\nntvf = TfidfVectorizer(use_idf=True, min_df=0.05, max_df=0.95, ngram_range=(1,1), sublinear_tf=True)\nntvf_features = ntvf.fit_transform(negative_reviews)\n\n# view feature set dimensions\nprint(ptvf_features.shape, ntvf_features.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dd443ce1fd9cd5e53cc27b110766a62a12964635"
      },
      "cell_type": "markdown",
      "source": "From the preceding output dimensions, you can see that we have filtered out a lot of the features we used previously when building our classification models by making min_df to be 0.05 and max_df to be 0.95. This is to speed up the topic modeling process and remove features that either occur too much or too rarely.\n\n#### Topic Modeling on Reviews\n\nThe NMF class from scikit-learn will help us with topic modeling. We also use pyLDAvis for building interactive visualizations of topic models. The core principle behind Non-Negative Matrix Factorization (NNMF) is to apply matrix decomposition (similar to SVD) to a non-negative feature matrix X such that the decomposition can be represented as X ≈ WH where W & H are both non-negative matrices which if multiplied should approximately re-construct the feature matrix X. A cost function like L2 norm can be used for getting this approximation. Let’s now apply NNMF to get 15 topics from our positive sentiment reviews. We will also leverage the functions to display the results by topics in a clean format."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f15a62989db00766bf509499f6170318d610d4f"
      },
      "cell_type": "code",
      "source": "pyLDAvis.enable_notebook()\ntotal_topics = 10\n\n# build topic model on positive sentiment review features\npos_nmf = NMF(n_components=total_topics, random_state=101, alpha=0.1, l1_ratio=0.2)\npos_nmf.fit(ptvf_features)\n\n# extract features and component weights\npos_feature_names = ptvf.get_feature_names()\npos_weights = pos_nmf.components_\n\n# extract and display topics and their components\npos_topics = get_topics_terms_weights(pos_weights, pos_feature_names)\nprint_topics_udf(topics=pos_topics,\n                 total_topics=total_topics,\n                 num_terms=15,\n                 display_weights=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d969db521d32a8b647a556b12e7952f4b7b0bb10"
      },
      "cell_type": "markdown",
      "source": "#### Visualize topics for positive reviews\n\nYou can leverage [pyLDAvis](https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf) now to visualize these topics in an interactive visualization."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a66eb0a18a6f33769e5f98b99371c1128a029711"
      },
      "cell_type": "code",
      "source": "pyLDAvis.sklearn.prepare(pos_nmf, ptvf_features, ptvf, R=15)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9160b39578bc43de4212ec48d592b379c03cb1c2"
      },
      "cell_type": "markdown",
      "source": "From the topics and the terms, we can see terms like movie cast, actors, performance, play, characters, music, wonderful, good, and so on have contributed toward positive sentiment in various topics. This is quite interesting and gives you a good insight into the components of the reviews that contribute toward positive sentiment of the reviews. \n\nThis visualization is completely interactive if you are using the jupyter notebook and you can click on any of the bubbles representing topics in the Intertopic Distance Map on the left and see the most relevant terms in each of the topics in the right bar chart.\n\nThe plot on the left is rendered using Multi-dimensional Scaling (MDS). Similar topics should be close to one another and dissimilar topics should be far apart. The size of each topic bubble is based on the frequency of that topic and its components in the overall corpus.\n\nThe visualization on the right shows the top terms. When no topic it selected, it shows the top 15 most salient topics in the corpus. A term's saliency is defined as a measure of how frequently the term appears the corpus and its distinguishing factor when used to distinguish between topics. When some topic is selected, the chart changes to shows the top 15 most relevant terms for that topic. The relevancy metric is controlled by λ, which can be changed based on a slider on top of the bar chart.\n\n#### Display and visualize topics for negative reviews\n\nFrom the topics and the terms, we can see terms like waste, time, money, crap, plot, terrible, acting, and so on have contributed toward negative sentiment in various topics. Of course, there are high chances of overlap between topics from positive and negative sentiment reviews, but there will be distinguishable, distinct topics that further help us with interpretation and causal analysis."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2829cf0fc614a47044a9160ff8fef91077b686e"
      },
      "cell_type": "code",
      "source": "# build topic model on negative sentiment review features\nneg_nmf = NMF(n_components=total_topics, random_state=101, alpha=0.1, l1_ratio=0.2)\nneg_nmf.fit(ntvf_features)      \n\n# extract features and component weights\nneg_feature_names = ntvf.get_feature_names()\nneg_weights = neg_nmf.components_\n\n# extract and display topics and their components\nneg_topics = get_topics_terms_weights(neg_weights, neg_feature_names)\nprint_topics_udf(topics=neg_topics,\n                 total_topics=total_topics,\n                 num_terms=15,\n                 display_weights=True) \n\npyLDAvis.sklearn.prepare(neg_nmf, ntvf_features, ntvf, R=15)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c6d7936c502ffd9cfc08c9b595a6cb6942d5ca4"
      },
      "cell_type": "code",
      "source": "!pip3 install jinja --user --upgrade",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "338.496px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}