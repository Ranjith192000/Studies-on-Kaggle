{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Movie Reviews - Sentiment Analysis II\n",
    "In this notebook, we continu with focus on trying to analyze a large corpus of movie reviews and derive the sentiment, but now we apply deep learning methods.\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1200/1*Cvb9MOko4Sx-Ds8wB4bPbw.jpeg)]\n",
    "\n",
    "We cover a wide variety of techniques of deep leaning for analyzing sentiment, which include the following.\n",
    "- Newer supervised Deep Learning models\n",
    "- Advanced supervised Deep Learning models\n",
    "\n",
    "Besides looking at various approaches and models, we also focus on important aspects in the Machine Learning pipeline including text pre-processing, normalization, and in-depth analysis of models, including model interpretation and topic models. The key idea here is to understand how we tackle a problem like sentiment analysis on unstructured text, learn various techniques, models and understand how to interpret the results. This will enable you to use these methodologies in the future on your own datasets. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-classify-Sentiment?\" data-toc-modified-id=\"How-to-classify-Sentiment?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>How to classify Sentiment?</a></span></li></ul></li><li><span><a href=\"#Preparing-environment-and-data\" data-toc-modified-id=\"Preparing-environment-and-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preparing environment and data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-and-Setting-Up-Dependencies\" data-toc-modified-id=\"Import-and-Setting-Up-Dependencies-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Import and Setting Up Dependencies</a></span></li><li><span><a href=\"#Text-Pre-Processing-and-Normalization\" data-toc-modified-id=\"Text-Pre-Processing-and-Normalization-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Text Pre-Processing and Normalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-Text---strip-HTML\" data-toc-modified-id=\"Cleaning-Text---strip-HTML-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Cleaning Text - strip HTML</a></span></li><li><span><a href=\"#Removing-accented-characters\" data-toc-modified-id=\"Removing-accented-characters-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Removing accented characters</a></span></li><li><span><a href=\"#Expanding-Contractions\" data-toc-modified-id=\"Expanding-Contractions-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Expanding Contractions</a></span></li><li><span><a href=\"#Removing-Special-Characters\" data-toc-modified-id=\"Removing-Special-Characters-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Removing Special Characters</a></span></li><li><span><a href=\"#Lemmatizing-text\" data-toc-modified-id=\"Lemmatizing-text-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Lemmatizing text</a></span></li><li><span><a href=\"#Removing-Stopwords\" data-toc-modified-id=\"Removing-Stopwords-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span>Removing Stopwords</a></span></li><li><span><a href=\"#Normalize-text-corpus---tying-it-all-together\" data-toc-modified-id=\"Normalize-text-corpus---tying-it-all-together-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;</span>Normalize text corpus - tying it all together</a></span></li></ul></li><li><span><a href=\"#Topics-Help-Functions\" data-toc-modified-id=\"Topics-Help-Functions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Topics Help Functions</a></span></li><li><span><a href=\"#Simplify-Get-Results\" data-toc-modified-id=\"Simplify-Get-Results-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Simplify Get Results</a></span></li><li><span><a href=\"#Load-and-normalize-data\" data-toc-modified-id=\"Load-and-normalize-data-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Load and normalize data</a></span></li></ul></li><li><span><a href=\"#Newer-Supervised-Deep-Learning-Models\" data-toc-modified-id=\"Newer-Supervised-Deep-Learning-Models-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Newer Supervised Deep Learning Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prediction-class-label-encoding\" data-toc-modified-id=\"Prediction-class-label-encoding-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Prediction class label encoding</a></span></li><li><span><a href=\"#Feature-Engineering-with-word-embeddings\" data-toc-modified-id=\"Feature-Engineering-with-word-embeddings-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Feature Engineering with word embeddings</a></span></li><li><span><a href=\"#Modeling-with-deep-neural-networks\" data-toc-modified-id=\"Modeling-with-deep-neural-networks-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Modeling with deep neural networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Building-Deep-neural-network-architecture\" data-toc-modified-id=\"Building-Deep-neural-network-architecture-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Building Deep neural network architecture</a></span></li><li><span><a href=\"#Model-Training,-Prediction-and-Performance-Evaluation\" data-toc-modified-id=\"Model-Training,-Prediction-and-Performance-Evaluation-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Model Training, Prediction and Performance Evaluation</a></span></li></ul></li></ul></li><li><span><a href=\"#Advanced-Supervised-Deep-Learning-Models\" data-toc-modified-id=\"Advanced-Supervised-Deep-Learning-Models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Advanced Supervised Deep Learning Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preparing-data\" data-toc-modified-id=\"Preparing-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Preparing data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenize-train-&amp;-test-datasets\" data-toc-modified-id=\"Tokenize-train-&amp;-test-datasets-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Tokenize train &amp; test datasets</a></span></li><li><span><a href=\"#Build-Vocabulary-Mapping-(word-to-index)\" data-toc-modified-id=\"Build-Vocabulary-Mapping-(word-to-index)-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Build Vocabulary Mapping (word to index)</a></span></li><li><span><a href=\"#Encode-and-Pad-datasets-&amp;-Encode-prediction-class-labels\" data-toc-modified-id=\"Encode-and-Pad-datasets-&amp;-Encode-prediction-class-labels-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Encode and Pad datasets &amp; Encode prediction class labels</a></span></li></ul></li><li><span><a href=\"#Build-the-LSTM-Model-Architecture\" data-toc-modified-id=\"Build-the-LSTM-Model-Architecture-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Build the LSTM Model Architecture</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-model-architecture\" data-toc-modified-id=\"Visualize-model-architecture-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Visualize model architecture</a></span></li></ul></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Train the model</a></span></li><li><span><a href=\"#Predict-and-Evaluate-Model-Performance\" data-toc-modified-id=\"Predict-and-Evaluate-Model-Performance-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Predict and Evaluate Model Performance</a></span></li></ul></li><li><span><a href=\"#Analyzing-Sentiment-Causation\" data-toc-modified-id=\"Analyzing-Sentiment-Causation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Analyzing Sentiment Causation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interpreting-Predictive-Models\" data-toc-modified-id=\"Interpreting-Predictive-Models-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Interpreting Predictive Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analyze-Model-Prediction-Probabilities\" data-toc-modified-id=\"Analyze-Model-Prediction-Probabilities-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Analyze Model Prediction Probabilities</a></span></li><li><span><a href=\"#Interpreting-Model-Decisions\" data-toc-modified-id=\"Interpreting-Model-Decisions-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Interpreting Model Decisions</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The problem at hand is sentiment analysis or opinion mining, where we want to analyze some textual documents and predict their sentiment or opinion based on the content of these documents.\n",
    "\n",
    "A text corpus consists of multiple text documents and each document can be as simple as a single sentence to a complete document with multiple paragraphs. Textual data, in spite of being highly unstructured, can be classified into two major types of documents:\n",
    "- ***Factual/objective documents***: typically depict some form of statements or facts with no specific feelings or emotion attached to them. \n",
    "- ***Subjective documents***: text that expresses feelings, moods, emotions, and opinions.\n",
    "\n",
    "Typically sentiment analysis seems to work best on subjective text, where people express opinions, feelings, and their mood. From a real-world industry standpoint, sentiment analysis is widely used to analyze corporate surveys, feedback surveys, social media data, and reviews for movies, places, commodities, and many more. The idea is to analyze and understand the reactions of people toward a specific entity and take insightful actions based on their sentiment.\n",
    "\n",
    "![image](https://www.kdnuggets.com/images/sentiment-fig-1-689.jpg)\n",
    "\n",
    "**Sentiment analysis** is also popularly known as **opinion analysis** or **opinion mining**. The key idea is to use techniques from text analytics, NLP, Machine Learning, and linguistics to extract important information or data points from unstructured text. This in turn can help us derive ***qualitative outputs*** like the overall sentiment being on a ***positive***, ***neutral***, or ***negative*** scale and ***quantitative outputs*** like the sentiment ***polarity***, ***subjectivity***, and ***objectivity*** proportions. \n",
    "\n",
    "**Sentiment polarity** is typically a numeric score that's assigned to both the positive and negative aspects of a text document based on subjective parameters like specific words and phrases expressing feelings and emotion. Neutral sentiment typically has 0 polarity since it does not express and specific sentiment, positive sentiment will have polarity > 0, and negative < 0. Of course, you can always change these thresholds based on the type of text you are dealing with.\n",
    "\n",
    "### How to classify Sentiment?\n",
    "![image](https://www.kdnuggets.com/images/sentiment-fig-2-532.jpg)\n",
    "__Machine Learning__:\n",
    "\n",
    "This approach, employes a machine-learning technique and diverse features to construct a classifier that can identify text that expresses sentiment. Nowadays, deep-learning methods are popular because they fit on data learning representations.\n",
    "\n",
    "__Lexicon-Based__:\n",
    "\n",
    "This method uses a variety of words annotated by polarity score, to decide the general assessment score of a given content. The strongest asset of this technique is that it does not require any training data, while its weakest point is that a large number of words and expressions are not included in sentiment lexicons.\n",
    "\n",
    "__Hybrid__:\n",
    "\n",
    "The combination of machine learning and lexicon-based approaches to address Sentiment Analysis is called Hybrid. Though not commonly used, this method usually produces more promising results than the approaches mentioned above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing environment and data\n",
    "### Import and Setting Up Dependencies\n",
    "\n",
    "Let’s load the necessary dependencies and settings before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    " \n",
    "datetimeFormat = '%Y-%m-%d %H:%M:%S.%f'\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('all', halt_on_error=False)\n",
    "\n",
    "import gensim\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "'''\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "'''\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dropout, Activation, Dense, Embedding, Dropout, SpatialDropout1D, LSTM, Bidirectional\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "#from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: NLP libraries which will be used include spacy, nltk, and gensim. Do remember to check that your installed nltk version is at least >= 3.2.4, otherwise, the ToktokTokenizer class may not be present. If you want to use a lower nltk version for some reason, you can use any other tokenizer like the default word_tokenize() based on the TreebankWordTokenizer. The version for gensim should be at least 2.3.0 and for spacy, the version used was 1.9.0. We recommend using the latest version of spacy which was recently released (version 2.x) as this has fixed several bugs and added several improvements.\n",
    "\n",
    "### Text Pre-Processing and Normalization\n",
    "\n",
    "An initial step in text and sentiment classification is pre-processing. A significant amount of techniques is applied to data in order to improvement of classification effectiveness. This enables standardization across a document corpus, which helps build meaningful features, to reduce dimensionality and reduce noise that can be introduced due to many factors like irrelevant symbols, special characters, XML and HTML tags, and so on.\n",
    "\n",
    "The main components in our text normalization pipeline are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Text - strip HTML\n",
    "Our text often contains unnecessary content like HTML tags, which do not add much value when analyzing sentiment. Hence we need to make sure we remove them before extracting features. The BeautifulSoup library does an excellent job in providing necessary functions for this. Our strip_html_tags(...) function enables in cleaning and stripping out HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing accented characters\n",
    "In our dataset, we are dealing with reviews in the English language so we need to make sure that characters with any other format, especially accented characters are converted and standardized into ASCII characters. A simple example would be converting é to e. Our remove_accented_chars(...) function helps us in this respect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanding Contractions\n",
    "In the English language, contractions are basically shortened versions of words or syllables. Contractions pose a problem in text normalization because we have to deal with special characters like the apostrophe and we also have to convert each contraction to its expanded, original form. Our expand_contractions(...) function uses regular expressions and various contractions mapped to expand all contractions in our text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Contraction Map\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Special Characters\n",
    "Simple regexes can be used to achieve this. Our function remove_special_characters(...) helps us remove special characters. In our code, we have retained numbers but you can also remove numbers if you do not want them in your normalized corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    text = re.sub(r'[^a-zA-z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing text\n",
    "**Word stems** are usually the base form of possible words that can be created by ***attaching affixes*** like prefixes and suffixes ***to the stem*** to create new words. This is known as **inflection**. The **reverse process** of obtaining the base form of a word is known as **stemming**. The nltk package offers a wide range of stemmers like the PorterStemmer and LancasterStemmer. **Lemmatization** is very similar to stemming, where we remove word affixes to get to the base form of a word. However the base form in this case is known as the **root word** but not the root stem. The difference being that ***the root word is always a lexicographically correct word***, present in the dictionary, but the root stem may not be so. We will be using lemmatization only in our normalization pipeline to retain lexicographically correct words. The function lemmatize_text(...) helps us with this aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords\n",
    "Words which have little or no significance especially when constructing meaningful features from text are also known as stopwords or stop words. These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a document corpus. Words like a, an, the, and so on are considered to be stopwords. There is no universal stopword list but we use a standard English language stopwords list from nltk. You can also add your own domain specific stopwords if needed. The function remove_stopwords(...) helps us remove stopwords and retain words having the most significance and context in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize text corpus - tying it all together\n",
    "\n",
    "We use all these components and tie them together in the following function called normalize_corpus(...), which can be used to take a document corpus as input and return the same corpus with cleaned and normalized text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters    \n",
    "        if special_char_removal:\n",
    "            doc = remove_special_characters(doc)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Help Functions\n",
    "\n",
    "We will also leverage some utility functions to support get and display topics from a corpus with their terms and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints components of all the topics obtained from topic modeling\n",
    "def print_topics_udf(topics, total_topics=1,\n",
    "                     weight_threshold=0.0001,\n",
    "                     display_weights=False,\n",
    "                     num_terms=None):\n",
    "    \n",
    "    for index in range(total_topics):\n",
    "        topic = topics[index]\n",
    "        topic = [(term, float(wt))\n",
    "                 for term, wt in topic]\n",
    "        topic = [(word, round(wt,2)) \n",
    "                 for word, wt in topic \n",
    "                 if abs(wt) >= weight_threshold]\n",
    "                     \n",
    "        if display_weights:\n",
    "            print('Topic #'+str(index+1)+' with weights')\n",
    "            print(topic[:num_terms]) if num_terms else topic\n",
    "        else:\n",
    "            print('Topic #'+str(index+1)+' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print(tw[:num_terms]) if num_terms else tw\n",
    "        print()\n",
    "        \n",
    "\n",
    "# Extracts topics with their terms and weights \n",
    "# Format is Topic N: [(term1, weight1), ..., (termn, weightn)]        \n",
    "def get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    sorted_indices = np.array([list(row[::-1]) \n",
    "                           for row \n",
    "                           in np.argsort(np.abs(weights))])\n",
    "    sorted_weights = np.array([list(wt[index]) \n",
    "                               for wt, index \n",
    "                               in zip(weights,sorted_indices)])\n",
    "    sorted_terms = np.array([list(feature_names[row]) \n",
    "                             for row \n",
    "                             in sorted_indices])\n",
    "    \n",
    "    topics = [np.vstack((terms.T, term_weights.T)).T \n",
    "              for terms, term_weights \n",
    "              in zip(sorted_terms, sorted_weights)]     \n",
    "    \n",
    "    return topics         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify Get Results\n",
    "Let's build a function to standardize the capture and exposure of the results of our models.\n",
    "\n",
    "As a classification problem, Sentiment Analysis uses the evaluation metrics of Precision, Recall, F-score, and Accuracy. Also, average measures like macro, micro, and weighted F1-scores are useful for multi-class problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, name, data, true_labels, target_names = ['positive', 'negative'], results=None, reasume=False):\n",
    "\n",
    "    if hasattr(model, 'layers'):\n",
    "        param = wtp_dnn_model.history.params\n",
    "        best = np.mean(wtp_dnn_model.history.history['val_acc'])\n",
    "        predicted_labels = model.predict_classes(data) \n",
    "        im_model = InMemoryModel(model.predict, examples=data, target_names=target_names)\n",
    "\n",
    "    else:\n",
    "        param = gs.best_params_\n",
    "        best = gs.best_score_\n",
    "        predicted_labels = model.predict(data).ravel()\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            im_model = InMemoryModel(model.predict_proba, examples=data, target_names=target_names)\n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            im_model = InMemoryModel(model.decision_function, examples=data, target_names=target_names)\n",
    "        \n",
    "    print('Mean Best Accuracy: {:2.2%}'.format(best))\n",
    "    print('-'*60)\n",
    "    print('Best Parameters:')\n",
    "    print(param)\n",
    "    print('-'*60)\n",
    "    \n",
    "    y_pred = model.predict(data).ravel()\n",
    "    \n",
    "    display_model_performance_metrics(true_labels, predicted_labels = predicted_labels, target_names = target_names)\n",
    "    if len(target_names)==2:\n",
    "        ras = roc_auc_score(y_true=true_labels, y_score=y_pred)\n",
    "    else:\n",
    "        roc_auc_multiclass, ras = roc_auc_score_multiclass(y_true=true_labels, y_score=y_pred, target_names=target_names)\n",
    "        print('\\nROC AUC Score by Classes:\\n',roc_auc_multiclass)\n",
    "        print('-'*60)\n",
    "\n",
    "    print('\\n\\n              ROC AUC Score: {:2.2%}'.format(ras))\n",
    "    prob, score_roc, roc_auc = plot_model_roc_curve(model, data, true_labels, label_encoder=None, class_names=target_names)\n",
    "    \n",
    "    interpreter = Interpretation(data, feature_names=cols)\n",
    "    plots = interpreter.feature_importance.plot_feature_importance(im_model, progressbar=False, n_jobs=1, ascending=True)\n",
    "    \n",
    "    r1 = pd.DataFrame([(prob, best, np.round(accuracy_score(true_labels, predicted_labels), 4), \n",
    "                         ras, roc_auc)], index = [name],\n",
    "                         columns = ['Prob', 'CV Accuracy', 'Accuracy', 'ROC AUC Score', 'ROC Area'])\n",
    "    if reasume:\n",
    "        results = r1\n",
    "    elif (name in results.index):        \n",
    "        results.loc[[name], :] = r1\n",
    "    else: \n",
    "        results = results.append(r1)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def roc_auc_score_multiclass(y_true, y_score, target_names, average = \"macro\"):\n",
    "\n",
    "  #creating a set of all the unique classes using the actual class list\n",
    "  unique_class = set(y_true)\n",
    "  roc_auc_dict = {}\n",
    "  mean_roc_auc = 0\n",
    "  for per_class in unique_class:\n",
    "    #creating a list of all the classes except the current class \n",
    "    other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "    #marking the current class as 1 and all other classes as 0\n",
    "    new_y_true = [0 if x in other_class else 1 for x in y_true]\n",
    "    new_y_score = [0 if x in other_class else 1 for x in y_score]\n",
    "    num_new_y_true = sum(new_y_true)\n",
    "\n",
    "    #using the sklearn metrics method to calculate the roc_auc_score\n",
    "    roc_auc = roc_auc_score(new_y_true, new_y_score, average = average)\n",
    "    roc_auc_dict[target_names[per_class]] = np.round(roc_auc, 4)\n",
    "    mean_roc_auc += num_new_y_true * np.round(roc_auc, 4)\n",
    "    \n",
    "  mean_roc_auc = mean_roc_auc/len(y_true)  \n",
    "  return roc_auc_dict, mean_roc_auc\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print('Accuracy:  {:2.2%} '.format(metrics.accuracy_score(true_labels, predicted_labels)))\n",
    "    print('Precision: {:2.2%} '.format(metrics.precision_score(true_labels, predicted_labels, average='weighted')))\n",
    "    print('Recall:    {:2.2%} '.format(metrics.recall_score(true_labels, predicted_labels, average='weighted')))\n",
    "    print('F1 Score:  {:2.2%} '.format(metrics.f1_score(true_labels, predicted_labels, average='weighted')))\n",
    "                        \n",
    "\n",
    "def train_predict_model(classifier,  train_features, train_labels,  test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    return predictions    \n",
    "\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, target_names):\n",
    "    \n",
    "    total_classes = len(target_names)\n",
    "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels)\n",
    "    cm_frame = pd.DataFrame(data=cm, \n",
    "                            columns=pd.MultiIndex(levels=[['Predicted:'], target_names], labels=level_labels), \n",
    "                            index=pd.MultiIndex(levels=[['Actual:'], target_names], labels=level_labels)) \n",
    "    print(cm_frame) \n",
    "    \n",
    "def display_classification_report(true_labels, predicted_labels, target_names):\n",
    "\n",
    "    report = metrics.classification_report(y_true=true_labels, y_pred=predicted_labels, target_names=target_names) \n",
    "    print(report)\n",
    "    \n",
    "def display_model_performance_metrics(true_labels, predicted_labels, target_names):\n",
    "    print('Model Performance metrics:')\n",
    "    print('-'*30)\n",
    "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
    "    print('\\nModel Classification report:')\n",
    "    print('-'*30)\n",
    "    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, target_names=target_names)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*30)\n",
    "    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, target_names=target_names)\n",
    "\n",
    "\n",
    "def plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n",
    "    \n",
    "    ## Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    if hasattr(clf, 'classes_'):\n",
    "        class_labels = clf.classes_\n",
    "    elif label_encoder:\n",
    "        class_labels = label_encoder.classes_\n",
    "    elif class_names:\n",
    "        class_labels = class_names\n",
    "    else:\n",
    "        raise ValueError('Unable to derive prediction classes, please specify class_names!')\n",
    "    n_classes = len(class_labels)\n",
    "   \n",
    "    if n_classes == 2:\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            prb = clf.predict_proba(features)\n",
    "            if prb.shape[1] > 1:\n",
    "                y_score = prb[:, prb.shape[1]-1] \n",
    "            else:\n",
    "                y_score = clf.predict(features).ravel()\n",
    "            prob = True\n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            y_score = clf.decision_function(features)\n",
    "            prob = False\n",
    "        else:\n",
    "            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(true_labels, y_score)      \n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = {0:3.2%})'.format(roc_auc), linewidth=2.5)\n",
    "        \n",
    "    elif n_classes > 2:\n",
    "        if  hasattr(clf, 'clfs_'):\n",
    "            y_labels = label_binarize(true_labels, classes=list(range(len(class_labels))))\n",
    "        else:\n",
    "            y_labels = label_binarize(true_labels, classes=class_labels)\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_score = clf.predict_proba(features)\n",
    "            prob = True\n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            y_score = clf.decision_function(features)\n",
    "            prob = False\n",
    "        else:\n",
    "            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n",
    "            \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_labels[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        ## Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_labels.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        ## Compute macro-average ROC curve and ROC area\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        ## Plot ROC curves\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:2.2%})'\n",
    "                       ''.format(roc_auc[\"micro\"]), linewidth=3)\n",
    "\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:2.2%})'\n",
    "                       ''.format(roc_auc[\"macro\"]), linewidth=3)\n",
    "\n",
    "        for i, label in enumerate(class_names):\n",
    "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:2.2%})'\n",
    "                                           ''.format(label, roc_auc[i]), linewidth=2, linestyle=':')\n",
    "        roc_auc = roc_auc[\"macro\"]   \n",
    "    else:\n",
    "        raise ValueError('Number of classes should be atleast 2 or more')\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0.0, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return prob, y_score, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and normalize data\n",
    "We can now load our IMDb movie reviews dataset, use the first 40,000 reviews for training models and the remaining 10,000 reviews as the test dataset to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'../input/movie_reviews.csv')\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# take a peek at the data\n",
    "display(dataset.head())\n",
    "\n",
    "# build train and test datasets\n",
    "train_reviews, test_reviews, train_sentiments, test_sentiments =\\\n",
    "    train_test_split(reviews, sentiments , test_size=0.20,  random_state=101)\n",
    "\n",
    "sample_review_ids = [7626, 3533, 9010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we will also use our normalization module to normalize our review datasets. This is a time-consuming operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time: 2019-01-31 15:05:52\n",
      "--------------------------------------------------------------------------------\n",
      "Normalize training dataset:\n",
      "Elapsed time: 1:07:42.532699\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Normalize test dataset:\n",
      "Elapsed time: 0:16:17.584540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print('Current date and time: {}'.format(now.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "# normalize training dataset\n",
    "print('-'*60)\n",
    "print('Normalize training dataset:')\n",
    "norm_train_reviews = normalize_corpus(train_reviews)\n",
    "diff = (datetime.datetime.now() - now)\n",
    "now = datetime.datetime.now()\n",
    "print('Elapsed time: {}\\n'.format(diff))\n",
    "\n",
    "# normalize test dataset\n",
    "print('-'*60)\n",
    "print('Normalize test dataset:')\n",
    "norm_test_reviews = normalize_corpus(test_reviews)\n",
    "diff = (datetime.datetime.now() - now)\n",
    "print('Elapsed time: {}\\n'.format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can improve with apply a DNN model.\n",
    "\n",
    "## Newer Supervised Deep Learning Models\n",
    "In this section, we will be building some deep neural networks and train them on some advanced text features based on word embeddings to build a text sentiment classification system.\n",
    "\n",
    "[![image](http://159.89.224.205/wp-content/uploads/2016/07/tumblr_inline_oabas5sThb1sleek4_540.png)](http://blog.aylien.com/leveraging-deep-learning-for-multilingual/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction class label encoding\n",
    "\n",
    "The following snippet helps us tokenize our movie reviews and also converts the text-based sentiment class labels into one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
      "Sample test label transformation:\n",
      "----------------------------------- \n",
      "Actual Labels: ['positive' 'positive' 'positive'] \n",
      "Encoded Labels: [1 1 1] \n",
      "One hot encoded Labels:\n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "num_classes=2  # positive -> 1, negative -> 0\n",
    "\n",
    "# tokenize train reviews & encode train labels\n",
    "tokenized_train = [tokenizer.tokenize(text) for text in norm_train_reviews]\n",
    "y_tr = le.fit_transform(train_sentiments)\n",
    "y_train = keras.utils.to_categorical(y_tr, num_classes)\n",
    "\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = [tokenizer.tokenize(text) for text in norm_test_reviews]\n",
    "y_ts = le.fit_transform(test_sentiments)\n",
    "y_test = keras.utils.to_categorical(y_ts, num_classes)\n",
    "\n",
    "# print class label encoding map and encoded labels\n",
    "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print('Sample test label transformation:\\n'+'-'*35,\n",
    "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_ts[:3], \n",
    "      '\\nOne hot encoded Labels:\\n', y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see from the preceding sample outputs how our sentiment class labels have been encoded into numeric representations, which in turn have been converted into one-hot encoded vectors. \n",
    "\n",
    "### Feature Engineering with word embeddings\n",
    "Basically, ***word embeddings*** can be used for **feature extraction** and **language modeling**. This representation tries to map each word or phrase into a complete numeric vector such that semantically similar words or terms tend to occur closer to each other and these can be quantified using these embeddings. \n",
    "\n",
    "The ***word2vec model*** was built by Google is perhaps one of the most popular neural network based probabilistic language models and can be used to learn distributed representational vectors for words. Word embeddings produced by word2vec involve taking in a corpus of text documents, representing words in a large high dimensional vector space such that each word has a corresponding vector in that space and similar words (even semantically) are located close to one another.\n",
    "\n",
    "We will be using the gensim framework to implement the same model of word2vec created by Google, on our corpus to extract features. Some of the important parameters in the model are explained briefly as follows:\n",
    "- **size**: Represents the feature vector size for each word in the corpus when transformed.\n",
    "- **window**: Sets the context window size specifying the length of the window of words to be taken into account as belonging to a single, similar context when training.\n",
    "- **min_count**: Specifies the minimum word frequency value needed across the corpus to consider the word as a part of the final vocabulary during training the model.\n",
    "- **sample**: Used to downsample the effects of words which occur very frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word2vec model\n",
    "w2v_num_features = 500\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150, min_count=10, sample=1e-3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in the corpus with at least 10 counts will essentially now be a vector itself of size 500. \n",
    "\n",
    "A question might arise in your mind now that so far, we had feature vectors for each complete document, but now we have vectors for each word. How do we represent entire documents now? We can do that using various aggregation and combinations. A simple scheme would be to use an averaged word vector representation, where we simply sum all the word vectors occurring in a document and then divide by the count of word vectors to represent an averaged word vector for the document. The following code enables us to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the previous function to generate averaged word vector representations on our two movie review datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model, num_features=w2v_num_features)\n",
    "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model, num_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We complete our generate embeddings by using the Global Vectors for Word Representation (GloVe) models, is an unsupervised model for obtaining word vector representations. Created at [Stanford University](https://nlp.stanford.edu/pubs/glove.pdf#_blank), this model is trained on various corpora like Wikipedia, Common Crawl, and Twitter and corresponding pre-trained word vectors are available that can be used for our analysis needs. \n",
    "\n",
    "The spacy library provided 384-dimensional word vectors trained on the Common Crawl corpus using the GloVe model. They provide a simple standard interface to get feature vectors of size 384 for each word as well as the averaged feature vector of a complete text document. \n",
    "\n",
    "Check on the [GloVe project site](https://nlp.stanford.edu/projects/glove) others pre-trained models and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model:> Train features shape: (40000, 500)  Test features shape: (10000, 500)\n",
      "GloVe model:> Train features shape: (40000, 384)  Test features shape: (10000, 384)\n"
     ]
    }
   ],
   "source": [
    "# feature engineering with GloVe model\n",
    "train_nlp = [nlp(item) for item in norm_train_reviews]\n",
    "train_glove_features = np.array([item.vector for item in train_nlp])\n",
    "\n",
    "test_nlp = [nlp(item) for item in norm_test_reviews]\n",
    "test_glove_features = np.array([item.vector for item in test_nlp])\n",
    "\n",
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, \n",
    "      ' Test features shape:', avg_wv_test_features.shape)\n",
    "print('GloVe model:> Train features shape:', train_glove_features.shape, \n",
    "      ' Test features shape:', test_glove_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with deep neural networks \n",
    "[![image](https://www.mdpi.com/algorithms/algorithms-09-00041/article_deploy/html/images/algorithms-09-00041-g002.png)](https://www.mdpi.com/1999-4893/9/2/41/htm)\n",
    "#### Building Deep neural network architecture\n",
    "We will be using a fully-connected four layer deep neural network (multi-layer perceptron or deep ANN) for our model. We call this a fully connected deep neural network (DNN) because neurons or units in each pair of adjacent layers are fully pairwise connected. These networks are also known as deep artificial neural networks (ANNs) or Multi-Layer Perceptrons (MLPs) since they have more than one hidden layer. The following function leverages keras on top of tensorflow to build the desired DNN model. We build a Sequential model, which helps us linearly stack our hidden and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(1024, activation='relu', input_shape=(num_input_features,)))\n",
    "    dnn_model.add(Dropout(0.5))\n",
    "    dnn_model.add(Dense(1024, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.5))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return dnn_model\n",
    "\n",
    "w2v_dnn = construct_deepnn_architecture(num_input_features=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not count the input layer usually in any deep architecture, hence our model will consist of three hidden layers of 512 neurons or units and one output layer with two units that will be used to either predict a positive or negative sentiment based on the input layer features.\n",
    "\n",
    "We use 1024, 1024 and 512 units for our hidden layers respectively and the **activation function relu** indicates a ***rectified linear unit***.  This function tries to solve the ***vanishing gradient problem***. This problem occurs when x > 0 and as x increases, where x is typically the input to a neuron, the gradient from sigmoids becomes really small (almost vanishing) but relu prevents this from happening. Besides this, it also ***helps with faster convergence of gradient descent***. \n",
    "\n",
    "We also use regularization in the network in the form of ***Dropout layers***. By adding a **dropout rates of 0.5 and 0.2**, it randomly sets 50% and 20% of the input feature units to 0 at each update during training the model. This form of regularization ***helps prevent overfitting the model***.\n",
    "\n",
    "The final output layer consists of two units with a ***softmax activation function***. The softmax function is basically a generalization of the **logistic function**, which can be used to represent a probability distribution over n possible class outcomes. In our case n = 2 where the class can either be positive or negative and the softmax probabilities will help us determine the same. \n",
    "\n",
    "The compile(...) method is used to configure the learning or training process of the DNN model before we actually train it. This involves providing a [***cost*** or ***loss function***](https://keras.io/losses/) in the loss parameter. This will be the goal or objective which the model will try to minimize. \n",
    "\n",
    "We will be using ***binary_crossentropy***, which helps us minimize the error or loss from the softmax output. We need an optimizer for helping us converge our model and minimize the loss or error function. Gradient descent or stochastic gradient descent is a popular optimizer. We will be using the ***rmsprop optimizer***, other option is [adam](https://arxiv.org/pdf/1412.6980v8.pdf ) also uses momentum where basically each update is based on not only the gradient computation of the current point but also includes a fraction of the previous update. This helps with faster convergence. \n",
    "\n",
    "Let's visualize our deep architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"626pt\" viewBox=\"0.00 0.00 218.00 626.00\" width=\"218pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 622)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-622 214,-622 214,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2342363587080 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2342363587080</title>\n",
       "<polygon fill=\"none\" points=\"6.5,-498.5 6.5,-544.5 203.5,-544.5 203.5,-498.5 6.5,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-517.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-498.5 57.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-521.5 113.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-498.5 113.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158.5\" y=\"-529.3\">(None, 500)</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-521.5 203.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158.5\" y=\"-506.3\">(None, 1024)</text>\n",
       "</g>\n",
       "<!-- 2342363587808 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2342363587808</title>\n",
       "<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 210,-461.5 210,-415.5 0,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-434.8\">Dropout</text>\n",
       "<polyline fill=\"none\" points=\"64,-415.5 64,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"64,-438.5 120,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"120,-415.5 120,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-446.3\">(None, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"120,-438.5 210,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-423.3\">(None, 1024)</text>\n",
       "</g>\n",
       "<!-- 2342363587080&#45;&gt;2342363587808 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2342363587080-&gt;2342363587808</title>\n",
       "<path d=\"M105,-498.366C105,-490.152 105,-480.658 105,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108.5,-471.607 105,-461.607 101.5,-471.607 108.5,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2342363588368 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2342363588368</title>\n",
       "<polygon fill=\"none\" points=\"6.5,-332.5 6.5,-378.5 203.5,-378.5 203.5,-332.5 6.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-351.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-332.5 57.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-355.5 113.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-332.5 113.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158.5\" y=\"-363.3\">(None, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-355.5 203.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158.5\" y=\"-340.3\">(None, 1024)</text>\n",
       "</g>\n",
       "<!-- 2342363587808&#45;&gt;2342363588368 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2342363587808-&gt;2342363588368</title>\n",
       "<path d=\"M105,-415.366C105,-407.152 105,-397.658 105,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108.5,-388.607 105,-378.607 101.5,-388.607 108.5,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2342363728864 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2342363728864</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 210,-295.5 210,-249.5 0,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-268.8\">Dropout</text>\n",
       "<polyline fill=\"none\" points=\"64,-249.5 64,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"64,-272.5 120,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"120,-249.5 120,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-280.3\">(None, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"120,-272.5 210,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-257.3\">(None, 1024)</text>\n",
       "</g>\n",
       "<!-- 2342363588368&#45;&gt;2342363728864 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2342363588368-&gt;2342363728864</title>\n",
       "<path d=\"M105,-332.366C105,-324.152 105,-314.658 105,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108.5,-305.607 105,-295.607 101.5,-305.607 108.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2342363728808 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2342363728808</title>\n",
       "<polygon fill=\"none\" points=\"6.5,-166.5 6.5,-212.5 203.5,-212.5 203.5,-166.5 6.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-185.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-166.5 57.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-189.5 113.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-166.5 113.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158.5\" y=\"-197.3\">(None, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-189.5 203.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158.5\" y=\"-174.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 2342363728864&#45;&gt;2342363728808 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>2342363728864-&gt;2342363728808</title>\n",
       "<path d=\"M105,-249.366C105,-241.152 105,-231.658 105,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108.5,-222.607 105,-212.607 101.5,-222.607 108.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2342445791496 -->\n",
       "<g class=\"node\" id=\"node6\"><title>2342445791496</title>\n",
       "<polygon fill=\"none\" points=\"3.5,-83.5 3.5,-129.5 206.5,-129.5 206.5,-83.5 3.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"35.5\" y=\"-102.8\">Dropout</text>\n",
       "<polyline fill=\"none\" points=\"67.5,-83.5 67.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"67.5,-106.5 123.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"123.5,-83.5 123.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-114.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"123.5,-106.5 206.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-91.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 2342363728808&#45;&gt;2342445791496 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>2342363728808-&gt;2342445791496</title>\n",
       "<path d=\"M105,-166.366C105,-158.152 105,-148.658 105,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108.5,-139.607 105,-129.607 101.5,-139.607 108.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2342445792448 -->\n",
       "<g class=\"node\" id=\"node7\"><title>2342445792448</title>\n",
       "<polygon fill=\"none\" points=\"10,-0.5 10,-46.5 200,-46.5 200,-0.5 10,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"35.5\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"61,-0.5 61,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"89\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"61,-23.5 117,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"89\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"117,-0.5 117,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158.5\" y=\"-31.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"117,-23.5 200,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158.5\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 2342445791496&#45;&gt;2342445792448 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>2342445791496-&gt;2342445792448</title>\n",
       "<path d=\"M105,-83.3664C105,-75.1516 105,-65.6579 105,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108.5,-56.6068 105,-46.6068 101.5,-56.6069 108.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2342363587472 -->\n",
       "<g class=\"node\" id=\"node8\"><title>2342363587472</title>\n",
       "<polygon fill=\"none\" points=\"53,-581.5 53,-617.5 157,-617.5 157,-581.5 53,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"105\" y=\"-595.8\">2342363587472</text>\n",
       "</g>\n",
       "<!-- 2342363587472&#45;&gt;2342363587080 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2342363587472-&gt;2342363587080</title>\n",
       "<path d=\"M105,-581.254C105,-573.363 105,-563.749 105,-554.602\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"108.5,-554.591 105,-544.591 101.5,-554.591 108.5,-554.591\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(w2v_dnn, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training, Prediction and Performance Evaluation\n",
    "We will be using the fit(...) function from keras for the training process and there are some parameters which you should be aware of:\n",
    "- **epoch**: indicates one complete forward and backward pass of all the training examples through the network. \n",
    "- **batch_size**: indicates the total number of samples which are propagated through the DNN model at a time for one backward and forward pass for training the model and updating the gradient. \n",
    "- **validation_split**: we use 0.15 to extract 15% of the training data and use it as a validation dataset for evaluating the performance at each epoch. \n",
    "- **shuffle**: helps shuffle the samples in each epoch when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34000 samples, validate on 6000 samples\n",
      "Epoch 1/15\n",
      " - 86s - loss: 0.3164 - acc: 0.8690 - val_loss: 0.2995 - val_acc: 0.8845\n",
      "Epoch 2/15\n",
      " - 38s - loss: 0.2937 - acc: 0.8799 - val_loss: 0.2974 - val_acc: 0.8803\n",
      "Epoch 3/15\n",
      " - 39s - loss: 0.2886 - acc: 0.8829 - val_loss: 0.2969 - val_acc: 0.8845\n",
      "Epoch 4/15\n",
      " - 39s - loss: 0.2822 - acc: 0.8852 - val_loss: 0.2905 - val_acc: 0.8842\n",
      "Epoch 5/15\n",
      " - 40s - loss: 0.2810 - acc: 0.8866 - val_loss: 0.2876 - val_acc: 0.8840\n",
      "Epoch 6/15\n",
      " - 43s - loss: 0.2741 - acc: 0.8899 - val_loss: 0.2881 - val_acc: 0.8817\n",
      "Epoch 7/15\n",
      " - 43s - loss: 0.2725 - acc: 0.8893 - val_loss: 0.2882 - val_acc: 0.8833\n",
      "Epoch 8/15\n",
      " - 43s - loss: 0.2657 - acc: 0.8920 - val_loss: 0.2863 - val_acc: 0.8832\n",
      "Epoch 9/15\n",
      " - 45s - loss: 0.2598 - acc: 0.8926 - val_loss: 0.2932 - val_acc: 0.8847\n",
      "Epoch 10/15\n",
      " - 44s - loss: 0.2511 - acc: 0.8948 - val_loss: 0.2999 - val_acc: 0.8792\n",
      "Epoch 11/15\n",
      " - 44s - loss: 0.2501 - acc: 0.8963 - val_loss: 0.3004 - val_acc: 0.8845\n",
      "Epoch 12/15\n",
      " - 44s - loss: 0.2448 - acc: 0.8976 - val_loss: 0.3099 - val_acc: 0.8817\n",
      "Epoch 13/15\n",
      " - 46s - loss: 0.2387 - acc: 0.9013 - val_loss: 0.3128 - val_acc: 0.8802\n",
      "Epoch 14/15\n",
      " - 50s - loss: 0.2321 - acc: 0.9034 - val_loss: 0.3177 - val_acc: 0.8783\n",
      "Epoch 15/15\n",
      " - 45s - loss: 0.2298 - acc: 0.9033 - val_loss: 0.3174 - val_acc: 0.8793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22164be38d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "track = w2v_dnn.fit(avg_wv_train_features, y_train, epochs=5, batch_size=batch_size, shuffle=True, \n",
    "                    validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create and use a simple function to plot the loss and accurancy by epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_acc_plot():\n",
    "    loss = track.history[\"loss\"]\n",
    "    acc = track.history[\"acc\"]\n",
    "    ep = list(range(len(loss)))\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 7))\n",
    "    l = fig.add_subplot(121)\n",
    "    plt.plot(ep, loss)\n",
    "    plt.xlabel(\"#epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "\n",
    "    a = fig.add_subplot(122)\n",
    "    plt.plot(ep, acc)\n",
    "    plt.xlabel(\"#epochs\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "loss_acc_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model performance on the test review word2vec features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marce\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy:  88.71% \n",
      "Precision: 88.72% \n",
      "Recall:    88.71% \n",
      "F1 Score:  88.71% \n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.89      0.88      0.89      4959\n",
      "   negative       0.88      0.90      0.89      5041\n",
      "\n",
      "avg / total       0.89      0.89      0.89     10000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       4353      606\n",
      "        negative        523     4518\n"
     ]
    }
   ],
   "source": [
    "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
    "predictions = le.inverse_transform(y_pred) \n",
    "print('-'*60)\n",
    "display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                  target_names=['positive', 'negative'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results is close to 89% in all metrics. As you see, the results were improve with the increase of epochs, but you need take care with the overfitting, maybe need try others DNN configurations to you can get better results! \n",
    "\n",
    "Let's see how our DNN model perform with our GloVe based features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34000 samples, validate on 6000 samples\n",
      "Epoch 1/15\n",
      " - 45s - loss: 0.6722 - acc: 0.5969 - val_loss: 0.6154 - val_acc: 0.6685\n",
      "Epoch 2/15\n",
      " - 39s - loss: 0.6403 - acc: 0.6266 - val_loss: 0.6130 - val_acc: 0.6843\n",
      "Epoch 3/15\n",
      " - 37s - loss: 0.6427 - acc: 0.6156 - val_loss: 0.6238 - val_acc: 0.6897\n",
      "Epoch 4/15\n",
      " - 39s - loss: 0.6271 - acc: 0.6359 - val_loss: 0.5833 - val_acc: 0.7092\n",
      "Epoch 5/15\n",
      " - 38s - loss: 0.6150 - acc: 0.6423 - val_loss: 0.5909 - val_acc: 0.7095\n",
      "Epoch 6/15\n",
      " - 37s - loss: 0.6270 - acc: 0.6294 - val_loss: 0.5988 - val_acc: 0.6878\n",
      "Epoch 7/15\n",
      " - 36s - loss: 0.6161 - acc: 0.6424 - val_loss: 0.5835 - val_acc: 0.7167\n",
      "Epoch 8/15\n",
      " - 36s - loss: 0.6041 - acc: 0.6681 - val_loss: 0.5604 - val_acc: 0.7187\n",
      "Epoch 9/15\n",
      " - 37s - loss: 0.5893 - acc: 0.6849 - val_loss: 0.6068 - val_acc: 0.6515\n",
      "Epoch 10/15\n",
      " - 37s - loss: 0.5876 - acc: 0.6824 - val_loss: 0.5772 - val_acc: 0.7177\n",
      "Epoch 11/15\n",
      " - 35s - loss: 0.5974 - acc: 0.6718 - val_loss: 0.5857 - val_acc: 0.7212\n",
      "Epoch 12/15\n",
      " - 36s - loss: 0.6008 - acc: 0.6577 - val_loss: 0.5652 - val_acc: 0.7177\n",
      "Epoch 13/15\n",
      " - 37s - loss: 0.5814 - acc: 0.6839 - val_loss: 0.5897 - val_acc: 0.7080\n",
      "Epoch 14/15\n",
      " - 36s - loss: 0.5727 - acc: 0.6986 - val_loss: 0.5608 - val_acc: 0.7253\n",
      "Epoch 15/15\n",
      " - 36s - loss: 0.5713 - acc: 0.6991 - val_loss: 0.5750 - val_acc: 0.7310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22164b9ff28>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dnn = construct_deepnn_architecture(num_input_features=384)\n",
    "\n",
    "batch_size = 64\n",
    "track = glove_dnn.fit(train_glove_features, y_train, epochs=5, batch_size=batch_size, shuffle=True, \n",
    "                      validation_split=0.1, verbose=2)\n",
    "\n",
    "loss_acc_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this model was somewhat lower given the reduction of inputs, however it seems more resistant to overfitting, it allow us to observe the potential in using pre-trained models. \n",
    "\n",
    "Let's take a look at the behavior of this model versus the test base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marce\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy:  72.87% \n",
      "Precision: 72.91% \n",
      "Recall:    72.87% \n",
      "F1 Score:  72.86% \n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.72      0.75      0.73      4959\n",
      "   negative       0.74      0.71      0.73      5041\n",
      "\n",
      "avg / total       0.73      0.73      0.73     10000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       3698     1261\n",
      "        negative       1452     3589\n"
     ]
    }
   ],
   "source": [
    "y_pred = glove_dnn.predict_classes(test_glove_features)\n",
    "predictions = le.inverse_transform(y_pred) \n",
    "print('-'*60)\n",
    "display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                  target_names=['positive', 'negative'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Supervised Deep Learning Models\n",
    "\n",
    "In this section we will use a more advanced models than your regular fully connected deep networks, a recurrent neural networks (RNNs) and long short term memory networks (LSTMs) which also considers the sequence of data (words,events, and so on). More over, we conclude with Bidirectional lstms, that's keep the contextual information in both directions.\n",
    "\n",
    "![image](http://thelillysblog.com/images/architecture-nn2.jpg)\n",
    "\n",
    "### Preparing data\n",
    "\n",
    "We will start with the procedures to preparing the data for our needs on the RNN and LSTM.\n",
    "\n",
    "#### Tokenize train & test datasets\n",
    "\n",
    "The following snippet helps us tokenize our movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [tokenizer.tokenize(text) for text in norm_train_reviews]\n",
    "tokenized_test = [tokenizer.tokenize(text) for text in norm_test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary Mapping (word to index)\n",
    "For feature engineering, we will be creating word embeddings. Word embeddings tend to vectorize text documents into fixed sized vectors such that these vectors try to capture contextual and semantic information.\n",
    "\n",
    "For generating embeddings, we will use the Embedding layer from keras, which requires documents to be represented as tokenized and numeric vectors. We already have tokenized text vectors, so we would need to convert them into numeric representations. Besides this, we would also need the vectors to be of uniform size even though the tokenized text reviews will be of variable length due to the difference in number of tokens in each review. For this, one strategy could be to take the length of the longest review (with maximum number of tokens\\words) and set it as the vector size, let's call this max_len. Reviews of shorter length can be padded with a PAD term in the beginning to increase their length to max_len.\n",
    "\n",
    "We would need to create a word to index vocabulary mapping for representing each tokenized text review in a numeric form. Do note you would also need to create a numeric mapping for the padding term which we shall call PAD_INDEX and assign it the numeric index of 0. For unknown terms, in case they are encountered later on in the test dataset or newer, previously unseen reviews, we would need to assign it to some index too. This would be because we will vectorize, engineer features, and build models only on the training data. Hence, if some new term should come up in the future, we will consider it as an out of vocabulary (OOV) term and assign it to a constant index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 85359\n",
      "Sample slice of vocabulary map:\n",
      " {'live': 11, 'action': 12, 'film': 13, 'trilogy': 14, 'direct': 15, 'peter': 16, 'jackson': 17, 'undoubtably': 18, 'far': 19, 'good': 20}\n"
     ]
    }
   ],
   "source": [
    "# build word to index vocabulary\n",
    "token_counter = Counter([token for review in tokenized_train for token in review])\n",
    "vocab_map = {item[0]: index+1 for index, item in enumerate(dict(token_counter).items())}\n",
    "max_index = np.max(list(vocab_map.values()))\n",
    "vocab_map['PAD_INDEX'] = 0\n",
    "vocab_map['NOT_FOUND_INDEX'] = max_index+1\n",
    "vocab_size = len(vocab_map)\n",
    "\n",
    "# view vocabulary size and part of the vocabulary map\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Sample slice of vocabulary map:\\n', dict(list(vocab_map.items())[10:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that we have used all the terms found in training dataset in our vocabulary. As alternative, you can easily filter and use more relevant terms here, based on their frequency, by using the most_common(count) function from Counter and taking the first count terms from the list of unique terms in the training corpus.\n",
    "\n",
    "#### Encode and Pad datasets & Encode prediction class labels\n",
    "\n",
    "The following snippet helps us encode and pad our movie reviews encode the tokenized text reviews based on the previous vocab_map. Also converts the text-based sentiment class labels into one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of train review vectors: 1442\n",
      "Train review vectors shape: (40000, 1442)  Test review vectors shape: (10000, 1442)\n"
     ]
    }
   ],
   "source": [
    "# get max length of train corpus and initialize label encoder\n",
    "le = LabelEncoder()\n",
    "num_classes=2 # positive -> 1, negative -> 0\n",
    "max_len = np.max([len(review) for review in tokenized_train])\n",
    "\n",
    "## Train reviews data corpus\n",
    "# Convert tokenized text reviews to numeric vectors\n",
    "train_X = [[vocab_map[token] for token in tokenized_review] for tokenized_review in tokenized_train]\n",
    "train_X = sequence.pad_sequences(train_X, maxlen=max_len) # pad \n",
    "## Train prediction class labels\n",
    "# Convert text sentiment labels (negative\\positive) to binary encodings (0/1)\n",
    "train_y = le.fit_transform(train_sentiments)\n",
    "\n",
    "## Test reviews data corpus\n",
    "# Convert tokenized text reviews to numeric vectors\n",
    "test_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map['NOT_FOUND_INDEX'] \n",
    "           for token in tokenized_review] \n",
    "              for tokenized_review in tokenized_test]\n",
    "test_X = sequence.pad_sequences(test_X, maxlen=max_len)\n",
    "## Test prediction class labels\n",
    "# Convert text sentiment labels (negative\\positive) to binary encodings (0/1)\n",
    "test_y = le.transform(test_sentiments)\n",
    "\n",
    "# view vector shapes\n",
    "print('Max length of train review vectors:', max_len)\n",
    "print('Train review vectors shape:', train_X.shape, ' Test review vectors shape:', test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the LSTM Model Architecture\n",
    "Let's introducing the ***Embedding layer*** and coupling it with the deep network architecture based on ***LSTMs***.\n",
    "\n",
    "The **Embedding layer** helps us generate the word embeddings from scratch. This layer is also initialized with some weights initially and this gets updated based on our optimizer similar to weights on the neuron units in other layers when the network tries to minimize the loss in each epoch. Thus, the embedding layer tries to optimize its weights such that we get the best word embeddings which will generate minimum error in the model and also capture semantic similarity and relationships among words.\n",
    "\n",
    "**LSTMs** basically try to overcome the shortcomings of RNN models especially with regard to handling long term dependencies and problems which occur when the weight matrix associated with the units/neurons become too small,***leading to vanishing gradient***, or too large, ***leading to exploding gradient***. The RNN units usually have a chain of repeating modules such that the module has a simple structure of having maybe one layer with the tanh activation. LSTMs are also a special type of RNN, having a similar structure but the LSTM unit has four neural network layers instead of just one. A **Bidirectional LSTM Layer** connects two hidden layers of opposite directions to the same output.\n",
    "![image](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n",
    "In the diagram below, the notation t indicates one time step, C depicts the cell states, and h indicates the hidden states. The gates i, f, o and c̅ help in removing or adding information to the cell state. The gates i, f and o represent the input, output and forget gates respectively and each of them are modulated by the sigmoid layer which outputs numbers from 0 to 1 controlling how much of the output from these gates should pass. Thus this helps is protecting and controlling the cell state.\n",
    "![image](https://i.stack.imgur.com/aTDpS.png)\n",
    "For a detailed work flow of how information flows through the LSTM cell consult the [Christopher Olah’s blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "The final layer in our deep network is the Dense layer with 1 unit and the sigmoid activation function. We basically use the binary_crossentropy function with the adam optimizer since this is a binary classification problem and the model will ultimately predict a 0 or a 1, which we can decode back to a negative or positive sentiment prediction with our label encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 64 # total LSTM units\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.3))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1442, 128)         10925952  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 1442, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 10,975,425\n",
      "Trainable params: 10,975,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"377pt\" viewBox=\"0.00 0.00 300.00 377.00\" width=\"300pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 373)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-373 296,-373 296,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2331314908800 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2331314908800</title>\n",
       "<polygon fill=\"none\" points=\"19,-249.5 19,-295.5 273,-295.5 273,-249.5 19,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"59\" y=\"-268.8\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"99,-249.5 99,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"99,-272.5 155,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"155,-249.5 155,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214\" y=\"-280.3\">(None, 1442)</text>\n",
       "<polyline fill=\"none\" points=\"155,-272.5 273,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214\" y=\"-257.3\">(None, 1442, 128)</text>\n",
       "</g>\n",
       "<!-- 2331314908912 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2331314908912</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 292,-212.5 292,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"59\" y=\"-185.8\">SpatialDropout1D</text>\n",
       "<polyline fill=\"none\" points=\"118,-166.5 118,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"118,-189.5 174,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"174,-166.5 174,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-197.3\">(None, 1442, 128)</text>\n",
       "<polyline fill=\"none\" points=\"174,-189.5 292,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-174.3\">(None, 1442, 128)</text>\n",
       "</g>\n",
       "<!-- 2331314908800&#45;&gt;2331314908912 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2331314908800-&gt;2331314908912</title>\n",
       "<path d=\"M146,-249.366C146,-241.152 146,-231.658 146,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"149.5,-222.607 146,-212.607 142.5,-222.607 149.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2331315860312 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2331315860312</title>\n",
       "<polygon fill=\"none\" points=\"32,-83.5 32,-129.5 260,-129.5 260,-83.5 32,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"59\" y=\"-102.8\">LSTM</text>\n",
       "<polyline fill=\"none\" points=\"86,-83.5 86,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"114\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"86,-106.5 142,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"114\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"142,-83.5 142,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201\" y=\"-114.3\">(None, 1442, 128)</text>\n",
       "<polyline fill=\"none\" points=\"142,-106.5 260,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201\" y=\"-91.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 2331314908912&#45;&gt;2331315860312 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2331314908912-&gt;2331315860312</title>\n",
       "<path d=\"M146,-166.366C146,-158.152 146,-148.658 146,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"149.5,-139.607 146,-129.607 142.5,-139.607 149.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2331315859696 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2331315859696</title>\n",
       "<polygon fill=\"none\" points=\"54,-0.5 54,-46.5 238,-46.5 238,-0.5 54,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"79.5\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"105,-0.5 105,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"105,-23.5 161,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"161,-0.5 161,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"199.5\" y=\"-31.3\">(None, 64)</text>\n",
       "<polyline fill=\"none\" points=\"161,-23.5 238,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"199.5\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 2331315860312&#45;&gt;2331315859696 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2331315860312-&gt;2331315859696</title>\n",
       "<path d=\"M146,-83.3664C146,-75.1516 146,-65.6579 146,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"149.5,-56.6068 146,-46.6068 142.5,-56.6069 149.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2331314909024 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2331314909024</title>\n",
       "<polygon fill=\"none\" points=\"94,-332.5 94,-368.5 198,-368.5 198,-332.5 94,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"146\" y=\"-346.8\">2331314909024</text>\n",
       "</g>\n",
       "<!-- 2331314909024&#45;&gt;2331314908800 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2331314909024-&gt;2331314908800</title>\n",
       "<path d=\"M146,-332.254C146,-324.363 146,-314.749 146,-305.602\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"149.5,-305.591 146,-295.591 142.5,-305.591 149.5,-305.591\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.summary())\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Training LSTMs on CPU is notoriously slow. Of course, a  GPU based Deep Learning environment or a cloud-based environment, like Google Cloud Platform or AWS on GPU, took approximately at least less than four times to train the same model. So I would recommend you choose GPU environment, especially when working with RNNs or LSTM based network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      " 1408/36000 [>.............................] - ETA: 4:18:00 - loss: 0.6924 - acc: 0.52 - ETA: 3:59:39 - loss: 0.6927 - acc: 0.50 - ETA: 3:54:08 - loss: 0.6924 - acc: 0.51 - ETA: 3:49:19 - loss: 0.6921 - acc: 0.52 - ETA: 3:46:19 - loss: 0.6919 - acc: 0.53 - ETA: 3:44:11 - loss: 0.6917 - acc: 0.53 - ETA: 3:42:57 - loss: 0.6917 - acc: 0.53 - ETA: 3:40:55 - loss: 0.6912 - acc: 0.54 - ETA: 3:39:32 - loss: 0.6909 - acc: 0.54 - ETA: 3:38:17 - loss: 0.6902 - acc: 0.55 - ETA: 3:37:24 - loss: 0.6903 - acc: 0.5518"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-6c97759588bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "track = model.fit(train_X, train_y, epochs=3, batch_size=batch_size, shuffle=True, validation_split=0.1, verbose=2)\n",
    "loss_acc_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the preceding output, we can see that just with five epochs we have decent validation accuracy, but like before, validation accuracy was nor better and the training accuracy starts shooting up indicating some over-fitting might be happening. Ways to overcome this include adding more data or by increasing the drouput rate. \n",
    "\n",
    "### Predict and Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict_classes(test_X)\n",
    "predictions = le.inverse_transform(pred_test.flatten())\n",
    "\n",
    "display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                  target_names=['positive', 'negative'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the other deep learning architecture we get close to 88% at all metrics, which is quite good! With more quality data, you can expect to get even better results. Try experimenting with different architectures and see if you get better results! \n",
    "\n",
    "Let's use a Bidirecional LSTM withou Word2Vec to see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1442)              0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 1442, 128)         10926080  \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 1442, 256)         263168    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1442, 256)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 128)               164352    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 11,353,729\n",
      "Trainable params: 11,353,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 36000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-2b4d62653962>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtrack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mloss_acc_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def movie_review_analysis(input_shape, vocab_len, embed_size):\n",
    "    \n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    embedding_layer = Embedding(vocab_len+1, embed_size)\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    X = Bidirectional(LSTM(units=128, return_sequences=True))(embeddings)\n",
    "    X = Dropout(rate=0.6)(X)\n",
    "    \n",
    "    X = Bidirectional(LSTM(units=64))(X)\n",
    "    X = Dropout(rate=0.3)(X)\n",
    "\n",
    "    X = Dense(units=1, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    return model\n",
    "\n",
    "model = movie_review_analysis(input_shape= (max_len,), vocab_len = vocab_size, embed_size=EMBEDDING_DIM)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "track = model.fit(train_X, train_y, epochs=2, batch_size=batch_size, shuffle=True, validation_split=0.1, verbose=2)\n",
    "loss_acc_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict_classes(test_X)\n",
    "predictions = le.inverse_transform(pred_test.flatten())\n",
    "\n",
    "display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                  target_names=['positive', 'negative'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Sentiment Causation\n",
    "\n",
    "Business and key stakeholders often perceive Machine Learning models as complex black boxes and poses the question, why should I trust your model? Explaining to them complex mathematical or theoretical concepts doesn't serve the purpose. Is there some way in which we can explain these models in an easy-to-interpret manner?\n",
    "\n",
    "[![image](https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/video_screenshot.png)](https://www.youtube.com/watch?v=hUnRCxnydCc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Predictive Models\n",
    "There are various ways to interpret the predictions made by our predictive sentiment classification models. We want to understand more into why a positive review was correctly predicted as having positive sentiment or a negative review having negative sentiment. Besides this, no model is a 100% accurate always, so we would also want to understand the reason for mis-classifications or wrong predictions. \n",
    "\n",
    "#### Analyze Model Prediction Probabilities\n",
    "Assuming our pipeline is in production, how do we use it for new movie reviews? Let's try to predict the sentiment for two new sample reviews, which were not used in training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(['the lord of the rings is an excellent movie', 'i hated the recent movie on tv, it was so bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classification pipeline predicts the sentiment of both the reviews correctly! This is a good start, but how do we interpret the model predictions? One way is to typically use the model prediction class probabilities as a measure of confidence. You can use the following code to get the prediction probabilities for our sample reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.predict_proba(['the lord of the rings is an excellent movie', \n",
    "                     'i hated the recent movie on tv, it was so bad']), columns=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can say that the first movie review has a prediction confidence or probability of 83% to have positive sentiment as compared to the second movie review with a 73% probability to have negative sentiment. \n",
    "\n",
    "#### Interpreting Model Decisions\n",
    "Besides prediction probabilities, we will be using the [skater framework](https://github.com/marcotcr/lime) for easy interpretation of the model decisions. First, to do this we define a helper function which takes in a document index, a corpus, its response predictions, and an explainer object and helps us with the our model interpretation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=classes)\n",
    "def interpret_classification_model_prediction(doc_index, norm_corpus, corpus, prediction_labels, explainer_obj):\n",
    "    # display model prediction and actual sentiments\n",
    "    print(\"Test document index: {index}\\nActual sentiment: {actual}\\nPredicted sentiment: {predicted}\"\n",
    "      .format(index=doc_index, actual=prediction_labels[doc_index],\n",
    "              predicted=model.predict([norm_corpus[doc_index]])))\n",
    "    # display actual review content\n",
    "    print(\"\\nReview:\", corpus[doc_index])\n",
    "    # display prediction probabilities\n",
    "    print(\"\\nModel Prediction Probabilities:\")\n",
    "    for probs in zip(classes, model.predict_proba([norm_corpus[doc_index]])[0]):\n",
    "        print(probs)\n",
    "    # display model prediction interpretation\n",
    "    exp = explainer.explain_instance(norm_corpus[doc_index], \n",
    "                                     model.predict_proba, num_features=10, \n",
    "                                     labels=[1])\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding snippet leverages skater to explain our text classifier to analyze its decision-making process in a global perspective. This is done by learning the model around the vicinity of the data point of interest X by sampling instances around X and assigning weightages based on their proximity to X. Thus, these locally learned linear models help in explaining complex models in a more easy to interpret way with class probabilities, contribution of top features to the class probabilities that aid in the decision making process. \n",
    "[![image](https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/lime.png)](https://arxiv.org/pdf/1602.04938.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index = 100 \n",
    "interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=test_X,\n",
    "                                         corpus=test_reviews, prediction_labels=test_sentiments,\n",
    "                                         explainer_obj=explainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show us the top 10 features and we can notice that our model performs quite well in this scenario. Besides this, the word great contributed the maximum to the positive probability of 0.16 and in fact if we had removed this word from our review text, the positive probability would have dropped significantly.\n",
    "\n",
    "Let's see a positive classification case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index = 2000\n",
    "interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=test_X,\n",
    "                                         corpus=test_reviews, prediction_labels=test_sentiments,\n",
    "                                         explainer_obj=explainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the content, the reviewer really liked this model and also it was a real cult classic among certain age groups. In our final analysis, we will look at the model interpretation of an example where the model makes a wrong prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index = 347 \n",
    "interpret_classification_model_prediction(doc_index=doc_index, norm_corpus=test_X, \n",
    "                                         corpus=test_reviews, prediction_labels=test_sentiments,\n",
    "                                         explainer_obj=explainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results tell us that the reviewer in fact shows signs of positive sentiment in the movie review, especially in parts where he\\she tells us that “I loved it. I still think the directing and cinematography are excellent, as is the music... Alan Rickman is great, a bit old perhaps, but he plays the role beautifully. And Elizabeth Spriggs, she is absolutely fantastic as always.” and feature words from the same have been depicted in the top features contributing to positive sentiment. The model interpretation also correctly identifies the aspects of the review contributing to negative sentiment like, “But it's really the script that has over the time started to bother me more and more.”. Hence, this is one of the more complex reviews which indicate both positive and negative sentiment and the final interpretation would be in the reader's hands. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "338.496px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
